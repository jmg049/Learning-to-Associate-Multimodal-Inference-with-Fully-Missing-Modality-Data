\label{sec:datasets}

\paragraph{\textbf{AudioSet}} is an audio event dataset consisting of an expanded ontology of 632 audio event classes and a collection of over 2M+ human-labelled 10-second sound clips taken from YouTube videos \cite{audioset}. Each entry in the dataset can have several event labels associated with it. There are two primary issues with the dataset. Firstly, the number of available entries constantly changes due to videos being made private or removed. Secondly, there is the possibility of modality bias \cite{tvqa} in how the audio events were labelled. Each entry was labelled by a human labeller who watched the video and listened to the audio. This means audio event labels were assigned using both the video and audio modalities.

A small subset of the complete dataset was used for this experiment. Initially, entries labelled as \textit{urban} or \textit{rural} were filtered. The resulting dataset contains 7276 samples with a 50/50 label split. Then, the labels were changed to \textit{speech} and \textit{non-speech} for further evaluation. Since each AudioSet entry contains multiple labels, the new labels were retrieved from the other labels associated with a given entry. The \textit{non-speech} label is defined as the absence of the \textit{speech} label. After changing the labels, the dataset becomes slightly imbalanced with a new split of 61/39 in favour of the \textit{speech} label. The dataset is split at random into 5093 (70\%) training, 1455 (20\%) validation and 728 (10\%) test entries.


\paragraph{\textbf{avMNIST}} is a extension of the avMNIST dataset proposed in \cite{vielzeuf2018centralnet}. The dataset's images are augmented to be associated with an audio recording of a person saying the digit. The version of MNIST used contains 70k entries, and the difference between the original avMNIST dataset and the one in this paper is that we have used 30k audio recordings taken from \cite{becker2018interpreting}. The audio recordings come from 60 male and female speakers. An audio sample of the same label was randomly selected for each image entry. Before use during training, the images were transformed by applying a Gaussian blur, a random colour invert, a random horizontal flip and a random vertical flip. This was done to train a more robust model. The parameters used for the transforms can be found in the Appendix. The classes are the digits 0-9, and the data is split at random into 49000 (70\%) training, 14000 (20\%) validation and 7000 (10\%) test entries.

\paragraph{\textbf{Kinetics-Sounds}} is a subset of the Kinetics \cite{kinetics} dataset containing 10-second clips obtained by filtering on a set of human action classes that potentially manifest both visually and aurally \cite{kinetics_sounds}, for example, "playing guitar" and "laughing". The dataset is split into 15k (70\%) training, 1.9k (20\%) validation and 1.9k (10\%) test entries. The complete list of classes can be found in the Appendix.


\paragraph{\textbf{MM-IMDb}} is a dataset containing two modalities, images and text, taken from the website IMDb\footnote{https://www.imdb.com/} \cite{arevalo2017gated}. We use this dataset to predict the genres associated with a given movie. Each movie in the dataset can be associated with multiple genres. This makes the task of predicting the genres a multi-label classification task. The dataset consists of 25956 movies with 23 possible genres. The dataset is split at random into 15552 (60\%) training, 2608 (10\%) validation and 7799 (30\%) test entries. The complete list of genres can be found in the Appendix.

\paragraph{\textbf{Common Data Preprocessing}}
All but one of the experiments carried out used the modalities of audio and video/images and, as a result, were pre-processed in the same way. This broad approach was carried out because, for all of the experiments conducted, we were not aiming to achieve state-of-the-art results for the individual tasks but rather investigated our approach as a general method of handling missing modality data. 

The video/image features were extracted using ResNet50 \cite{he2015deep}, as it has proven performance on various tasks. ResNet50 produced $400x1$ dimension features used as input to the video models. Eight frames were selected using a uniform random selection across all frames. The chosen frames were then centre-cropped and normalized before feature extraction. The resulting features were stored offline to speed up the training and evaluation procedure. 

The audio recordings were converted to Mel Spectrograms (the parameters used can be found in Section \ref{sec:app_mel_spect}). The resulting spectrograms were stored offline to speed up the training and evaluation procedure. 

The MM-IMDb dataset consists of movie poster images and a text description of the movie. The creators of the dataset have published the pre-processed features online, and no further processing was necessary. The image features were extracted using VGG16. Each movie poster was resized to $160x256$ pixels before being passed to VGG16. The text features were extracted using word2vec. However, before feature extraction, the vocabulary was intersected with the (lowercase) words from the movie plots. The final vocabulary contains 41,612 words.

