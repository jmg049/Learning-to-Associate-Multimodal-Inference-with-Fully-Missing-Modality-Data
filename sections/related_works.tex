
The following paragraphs provide details on handling partially missing modality data and generative and non-generative approaches to fully missing modality data. This section focuses on methods of handling missing modality data, we provide the related works that motivated C-MAMs in \Cref{sec:proposed_method}.

\textbf{Handling of Partially Missing Data}: Data loss is often partial, where corrupted or missing portions can impact model training efficacy. Several methods address this challenge. \citet{8273601} propose a multimodal autoencoder (MMAE) trained on complete data before randomly masking a modality to encourage cross-modal learning. Other approaches reconstruct missing data at the modality level, such as \citet{10.1007/978-3-030-03493-1_62}, who use a Cartesian Genetic Programming and Recurrent Neural Network (CGPRNN) with a sliding window to leverage RNN memory for signal restoration. Building on \cite{8273601}, \citet{NIPS2012_6cdd60ea} and \citet{9099281} employ stacked denoising autoencoders with white Gaussian noise to corrupt samples, evaluating their method by removing overlaid text from images.

Simpler methods involve discarding incomplete samples or padding missing data with zeros or averages \cite{10.1145/3395035.3425202, ma2021maximum, 9258396}. While effective, these methods do not preserve inter-modal correlations. \citet{10122560} propose the Efficient Multimodal Transformer with Dual Level Feature Restoration (EMT-DLFR), which integrates a transformer with a reconstruction network to generate missing sequences. EMT-DLFR improves over prior methods by reducing quadratic scaling costs and enhancing overall performance. Notably, when the percentage of missing data reaches 100\%, it effectively addresses the fully missing data problem.

Other studies leverage cross-modal signals for reconstruction. \citet{9563268} introduce an audio-driven GAN model that aligns audio with available video frames to restore missing video content. \citet{9842374} present an audio-haptic fused visual signal restoration (AHFVR) approach, employing semantic fusion of audio and haptic features, adversarial mapping to a shared latent space, and hierarchical knowledge distillation to enhance visual reconstruction. AHFVR demonstrates superior performance over single-modality and non-hierarchical methods in both benchmark datasets and real-world scenarios.

\textbf{Handling of Fully Missing Data - Non-generative Methods}: These approaches incorporate missing modality data into the training procedure of multimodal models. The intuition with these approaches is that if a model is given samples with missing data during training, it will be more capable of dealing with missing data during inference. \citet{10.1145/3394486.3403234} utilise a knowledge distillation framework for handling incomplete modalities. First, unimodal teacher models are trained using all available samples, including those with missing data. Then, the multimodal student network is trained using complete samples and the output of the teacher models. \citet{7993002}  propose a multi-hypergraph learning approach for handling incomplete data. The hypergraph model is trained on all modality data, whether it contains missing data or not. The authors do not report the degree of missing data; they only record what is recorded in the base dataset itself. \citet{ma2021maximum} propose a maximum likelihood-based approach that characterises the conditional distribution of the modality-complete data and the modality-missing data. The authors develop a generalised softmax function to implement the likelihood estimation efficiently. In this approach, missing data is handled during training. \citet{Matsuura_2018_ECCV_Workshops} propose a Bayesian Canonical correlation-based approach which estimates the relationships among the non-missing data and the feature space in the non-missing modality. Missing data is incorporated into the model's training via the likelihood function. \citet{QIAN2023443} uses contrastive learning and GAN augmentation to handle incomplete modality data. The GAN aspect of the model is not used to generate data for inference but to provide a balance of positive and negative pairs for contrastive learning. 

\textbf{Handling of Fully Missing Data - Generative Methods}:
These methods all reconstruct the missing modality data in some way, for example, full imputation of the missing modality or weight reconstruction. \citet{9755996} propose a UU-net autoencoder architecture to generate the missing modality. The first U-net handles intramodal fusion, while the second handles intermodal fusion. The purpose of the two networks is first to obtain unique feature representation and, secondly, to learn a shared latent space. Missing modalities are then generated by minimising the reconstruction error across both U-nets. Missing data is handled during the training process. \citet{Tran_2017_CVPR} propose a stacked residual autoencoder approach named Cascading Residual Autoencoder (CRA).
The approach is similar to those proposed in \cite{8273601,9099281}, except the denoising autoencoders are replaced with residual ones. Residual autoencoders output the difference between the input and output data rather than generating just the output. Then, the estimation is refined at each layer by stacking these residual autoencoders. The missing data is handled during training. A graph-based approach is proposed by \citet{10.1007/978-3-031-30675-4_19}. The authors construct a bipartite graph where samples and modalities are two types of nodes, and edges represent the observed modality value. Once the initial graph model is trained, it can be further trained to impute edge values for missing modalities. The model handles missing data during the training phase. \citet{YUAN2012622} propose a hybrid approach to handling missing data. They develop a multi-source feature learning model that learns a common feature space for the available modalities. They then use other imputation approaches (zeroing/averaging, k-nearest neighbours, singular value decomposition and expectation maximisation) alongside ensemble learning to perform decision-level fusion. \citet{smil} propose a weight-learning approach to generating missing modality data. The approach leverages Bayesian meta-learning to achieve both flexibility and efficiency. Missing modalities are approximated using a weighted sum of modality priors instead of directly generating the missing modality. This method approaches the problem of missing data during training. \citet{10.1145/3474085.3475585} propose a transformer-based feature reconstruction network. The network is trained to transform partially or entirely missing modality features into complete modalities. Complementary modality information is used to enhance the missing data under the guidance of the reconstruction loss. The transformation uses an attention mechanism to attend to the input modalities' missing portions. \citet{10.1145/3394486.3403182} propose a graph-based fusion method to enable learning on incomplete multimodal data. They construct a heterogeneous hypernode graph to model combinations of complete and incomplete modalities. The technique sees promising results without the need to impute modalities. \citet{8253467} propose a method which aims to exploit both semantic complementarity and similar distributions (the authors assume that modalities share identical distributions). To achieve this, they utilise a two-step process where an Isomorphic Linear Correlation Analysis (ILCA) is performed first. Then, this is followed by Identical Distribution Pursuit Completion to impute the missing data. \citet{10.1145/3219819.3219963} propose a deep adversarial learning approach which generates missing modalities. The authors formulate the problem as a conditional image generation task, using a GAN and an auxiliary adversarial loss function to generate high-quality missing modality images. Cross-partial multi-view networks (CPM-Nets) \cite{9258396} also utilise a GAN to impute missing information views. The network is trained to create a unified latent space, which is then used with the adversarial strategy to reconstruct a missing modality. \citet{10.1145/3581783.3611696} introduces Sample Less Learn More (SLLM), a technique that uses a frame feature restoration module to reconstruct features of deliberately omitted frames during training, thereby reducing computational costs in video action recognition. Unlike methods addressing missing modalities at inference time, SLLM focuses on mitigating performance impacts caused by intentionally reduced training data, achieving significant efficiency gains with minimal accuracy loss across various action recognition frameworks. The Multimodal Imagination Network (MMIN) proposed by \citet{zhao-etal-2021-missing} uses the CRA architecture proposed in \cite{Tran_2017_CVPR} to generate the multimodal embeddings created by the input modalities. They first train a baseline late fusion model, which is then used as a foundation model for the imagination network. They re-train the baseline model with the imagination network now included. The MMIN was evaluated on two datasets, focused on the sole task of multimodal sentiment analysis, and established a strong baseline for generating missing data in multimodal sentiment analysis. 

\textbf{Inference-Time Missing Modality Solutions and Modular Deployment Needs: } Despite extensive research on handling missing modalities during training, relatively few works have addressed inference-time reconstruction. Most approaches assume that models should be trained for robustness to missing data rather than developing methods for post-hoc adaptation. This assumption limits the flexibility of existing techniques, particularly in real-world settings where models may be deployed in environments where re-training is infeasible. Addressing missing modalities at inference time is essential for improving model longevity and adaptability without increasing training complexity.

Furthermore, many existing multimodal learning methods assume centralised computational resources, limiting their applicability in distributed environments such as federated learning, IoT, and multi-agent systems~\cite{NEURIPS2023_9156b0f6,adnan_cfl_2022}. The constraints of these environments necessitate \textbf{modular, targeted solutions} that can function independently of the primary multimodal model. Studies in federated learning have shown that \textbf{heterogeneous modality availability} across clients can hinder performance, requiring solutions that do not rely on centralised data access~\cite{pretrained_multimodal_decision_making_yunhao_24}. Our work addresses these gaps by introducing a \textbf{post-training, inference-time missing modality reconstruction framework} that enables adaptable deployment without requiring modifications to the original model.

