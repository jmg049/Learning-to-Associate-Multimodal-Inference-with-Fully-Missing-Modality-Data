The following sections detail experiments evaluating our proposed C-MAM approach. We start with toy datasets and simple multimodal models to establish a proof-of-concept. Next, we increase complexity by incorporating more data, modalities, and challenging classification tasks. Finally, we compare C-MAM against two state-of-the-art methods for missing modality data, EMT-DLFR for partially missing data and MMIN for fully missing data. Additionally, we assess performance under varying missing data percentages to understand how multimodal models leverage information from different modalities. Our evaluation spans six multimodal models across eight datasets with four modalities.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.99\textwidth]{imgs/C-MAM Architecture.pdf}
    \caption{C-MAM architecture and integration. The left side shows a standard multimodal model with modality-specific encoders, fusion, and classification. The middle illustrates a C-MAM for a missing modality case, where Modality C is reconstructed using A and B and compared against the ground-truth embedding with MSE. The right side demonstrates the integration of the trained C-MAM into the multimodal model, enabling inference-time reconstruction of missing modalities.}
    \label{fig:ACM_TOMM_MM_CMAM_ARCH}
\end{figure}
\begin{algorithm}[ht!]
    \small
\caption{Training Multimodal Model and C-MAM}
\label{alg:cmam_training}
\begin{algorithmic}[1]
\REQUIRE Dataset $D$, Multimodal Model $MM$, Missing modality $m'$, Available modality set $S$, Loss function $\mathcal{L}_{MAE}$

\FOR{epoch $= 1$ to $\text{num\_epochs\_MM}$}
\FOR{$(d_j, y_j) \in D$}
\STATE Compute modality embeddings $f_j = (E_1(m_{1j}), \dots, E_K(m_{Kj}))$
\STATE Compute fused representation $f_{fused} = F(f_j)$
\STATE Predict $\hat{y}_j = C(f_{fused})$
\STATE Compute loss: $\text{Loss}_{MM} = \mathcal{L}(y_j, \hat{y}_j)$
\STATE Update multimodal model
\ENDFOR
\ENDFOR

\STATE Initialise C-MAM with encoders $\tilde{E}$ and association network $A$

\FOR{epoch $= 1$ to $\text{num\_epochs\_CMAM}$}
\FOR{$(d_j, y_j) \in D$}
\STATE Compute target embedding $f_{m'} = E_{m'}^*(m_{m'j})$
\STATE Compute available embeddings $f_S = F(\{\tilde{E}_i(m_{ij}) | i \in S\})$
\STATE Predict missing modality embedding $\hat{f}_{m'} = A(f_S)$
\STATE Compute loss: $\text{Loss} = \mathcal{L}_{MAE}(f_{m'}, \hat{f}_{m'})$
\STATE Update C-MAM parameters
\ENDFOR
\ENDFOR
\RETURN Optimised $MM^*$ and C-MAM
\end{algorithmic}
\end{algorithm}
\subsection{Experiment 1: \textit{Toy} datasets}


\noindent \textbf{Experiment One} aims to assess the viability of our proposed approach using two datasets: a subset of AudioSet \cite{audioset} and AVMNIST \cite{vielzeuf2018centralnet}, a multimodal extension of the MNIST dataset.

\textbf{AudioSet}: The full AudioSet dataset comprises over 2M, 10s YouTube videos labeled with 632 audio event classes. For our purposes, we selected a subset focusing on entries labeled as containing speech, treating the absence of a speech label as indicative of non-speech. While we initially aimed for 10,000 entries, issues such as copyright notices and unavailable videos resulted in a final dataset of 7,276 entries, with a class imbalance of 61/39 in favor of the speech label. The subset was split into train, validation, and test sets (70-20-10). The \textbf{AudioSet Model} includes a video representation network using a pre-trained and frozen ResNet50 backbone \cite{7780459} followed by a linear layer, and an audio representation network composed of a convolution block (two stacked convolutional layers) and a linear layer. These unimodal features are fused and passed through three linear layers.

\textbf{AVMNIST}: AVMNIST is a multimodal extension of MNIST that includes audio clips of 60 speakers pronouncing digits, resulting in 70k entries. Each image is associated with a randomly (but balanced) selected speaker. Before training, images undergo Gaussian blur, random color inversion, random horizontal flip, and random vertical flip, while audio recordings are converted into Mel-Spectrograms. The dataset, containing ten classes, is split into train, validation, and test sets (70-20-10). The \textbf{AVMNIST Model} consists of an image representation network with two convolution blocks followed by max pooling and two linear layers, and an audio representation network with two convolution blocks with max pooling and one linear layer. The outputs of these networks are fused before being processed by three linear layers.

\subsection{Experiment 2: Advanced datasets}
\textbf{Experiment two} then aims to determine the approach's effectiveness on a more challenging set of tasks with some more modalities. Two more datasets were selected to test the C-MAM approach; a subset of the Kinetics dataset \cite{kinetics}, Kinetics-Sounds \cite{kinetics_sounds} and the MM-IMDb \cite{arevalo2017gated} movie genre classification dataset. 

\textbf{Kinetics-Sounds}: The Kinetics-Sounds dataset is a subset of the Kinetics dataset. It contains 19K 10-second clips, which were obtained by filtering the Kinetics dataset for human action classes that potentially manifest both visually and aurally, for example, "playing the guitar" and "laughing". The videos in the dataset were pre-processed by centre-cropping, normalisation, and uniform random frame selection (eight frames were sampled), and finally converted to a $400x1$ feature tensor by passing it to a pre-trained ResNet50 model. The dataset contains 26 classes and was split into a 70-20-10 train-validation-test split. \textbf{Kinetics-Sounds Model}: The Kinetics-Sounds video representation network uses a ResNet50 backbone network (pre-trained and frozen), followed by two linear layers. The audio representation network consists of three convolution blocks with average pooling after each block, followed by two linear layers. The multimodal model then consists of a fusion layer and two linear layers.

\textbf{MM-IMDb}: The MM-IMDb dataset contains two modalities: images and text. The data is taken from the IMDB\footnote{https://www.imdb.com} website. Each entry in the dataset includes a description of the movie, a promotional poster of the film and a set of genres associated with the movie. Each entry can be associated with multiple genres. The dataset contains 25.9K entries and 23 classes. The dataset contains the extracted features of both modalities. The image features were obtained by providing the images to a trained VGG16 model, and the text features were extracted using \textit{word2vec} (which was intersected with the vocabulary of the movie descriptions). The final vocabulary contains 41,612 words. The dataset was split into a 60-10-30 train-validation-test split. \textbf{MM-IMDb Model}: For the MM-IMDb dataset, the gated multimodal model architecture developed by \citet{arevalo2017gated} was used\footnote{We based our implementation on https://github.com/johnarevalo/gmu-mmimdb}. The model is composed of two max-out multilayer perceptrons \cite{pmlr-v28-goodfellow13} followed by a gated multimodal unit (GMU), which was inspired by recurrent architectures such as gated recurrent units (GRUs) or long short-term memory (LSTM) networks. The GMU handles the fusion of the modality feature representations before passing the fused representation to another multilayer perceptron for classification.

\subsection{Experiment 3: State-of-the-Art Models}
\textbf{Experiment three} then compares the C-MAM approach to two state-of-the-art publically available multimodal models. Each model is evaluated on two datasets from multimodal sentiment analysis. The aim of this experiment is two-fold; firstly, to compare the performance of the C-MAM with an established state-of-the-art method and secondly, to investigate whether or not the approach can be used in conjunction with other methods of addressing missing data. 


\textbf{Efficient Multimodal Transformer with Dual Level Feature Restoration (EMT-DLFR)}: The EMT-DLFR is a transformer-based model which performs low-level feature reconstruction during training to encourage the model to learn semantic information from incomplete data. The reconstruction network of the EMT-DLFR utilises Mutual Promotion Units (MPUs) to explore the inherent correlations between elements across two input sequences. The EMT-DLFR focuses on incomplete (partial) missing data. This makes a direct comparison between C-MAMs and the EMT-DLFR unfair in the majority of cases. However, one scenario where a direct comparison can be made is under 100\% partially missing data, equivalent to the modality being fully missing. Other research, \cite{hazarika2022analyzing}, also indicates that training a multimodal model to be more robust to missing data up to a percentage (for example, 30\%) can also improve the model's performance when higher percentages are missing. The code for the EMT-DLFR is available on Github \footnote{https://github.com/sunlicai/EMT-DLFR}. We do not modify the original code except to enable the models to return the modality feature representations and to use a C-MAM-generated feature when a modality is missing. We trained two versions of the EMT-DLFR, a baseline model with no missing data and a \textit{robust} version which was trained with 30\% missing data. \textbf{MOSI} \cite{mosi} \textbf{\& MOSEI} \cite{mosei}: The CMU-MOSI and CMU-MOSEI datasets are two MSA datasets containing the audio, video and text modalities. Both are frequently used in evaluating MSA models. The CMU-MOSI dataset includes 2,199 entries, while the CMU-MOSEI dataset contains 23,500 entries. Both datasets were pre-processed by the procedure followed in \cite{10122560}. Each entry in the datasets is labelled with a sentiment intensity score between -3 to +3 (MOSI) or -1 to +1 (MOSEI), indicating the sentiment (strongly negative to strongly positive) expressed in the multimedia content. The CMU-MOSI dataset is split into a 66-12-22 train-validation-test split, whereas the CMU-MOSEI is divided into a 75-10-15 train-validation-test split. 

\textbf{Multimodal Imagination Network}: The Multimodal Imagination Network (MMIN) is a widely used baseline model for handling fully missing modality data. It utilises an autoencoder-based imagination network to manage various uncertain missing modality conditions. The MMIN integrates audio, video, and text features for prediction. The autoencoder relies on the Cascade Residual Autoencoder (CRA) architecture proposed in \cite{Tran_2017_CVPR}, and employs Cycle Consistency Learning \cite{zhu2017unpaired} to generate representations of the missing modalities from the available modalities in the fused representation space. The latent representations from the CRA are collected to form the multimodal representation. The model's performance is assessed under all possible combinations of missing modalities. The effectiveness of MMIN is demonstrated using the IEMOCAP and MSP-IMPROV datasets. The code for MMIN is available on Github \footnote{https://github.com/AIM3-RUC/MMIN/}. We do not modify the original code except to enable the models to return the modality feature representations and to use a C-MAM-generated feature when a modality is missing. We follow the same evaluation procedure as the proposing paper, i.e. multiple runs for each model and dataset, using 10-12 folds for cross-validation. We use the baseline late-fusion model from the paper to compare the MMIN approach and the C-MAM approach, both built on that baseline. \textbf{IEMOCAP} \cite{busso2008iemocap}: The IEMOCAP dataset contains video recordings of 5 dyadic conversation sessions. Each recording has multiple scripted plays and spontaneous dialogues between a male and female speaker. In total, there are 10 speakers in the dataset. The main purpose of the dataset is to conduct emotion recognition. \ pre-processes the modalities cite{zhao-etal-2021-missing,liang2020semi,xu2019learning}. We use the dataset version which contains four emotion classes. \textbf{MSP-IMPROV} \cite{Busso_2017}: The MSP-IMPROV dataset is similar to the IEMOCAP dataset, including dyadic conversational sessions between two speakers. There are a total of twelve speakers. We follow the pre-processing steps outlined in \cite{zhao-etal-2021-missing}. There are four classes in the final dataset, \textit{happy}, \textit{anger}, \textit{sadness} and \textit{neutral}.

\subsection{C-MAMs Architectures}
Each C-MAM implemented in this paper follows the same design strategy: It includes a frozen trained modality-specific encoder for each input modality, which comes from the trained multimodal model. Then, a series of fully connected layers, with batch normalisation, and ReLU activation in between, are used to map the feature vector to the target modality feature. 

The parameters of the trained modality-specific encoders \textbf{are frozen} during C-MAM training, meaning only the association network is trained. It would be possible to fine-tune the modality-specific encoders during C-MAM training, but this requires more training time and does not guarantee an improvment in performance (see \Cref{sec:encoders}) of the Appendix. Each C-MAM is trained using MSE as the loss function. Further information on the C-MAM architectures is available in the \Cref{sec:parameter_counts} of the Appendix. 

\subsection{Model Training and Evaluation}

Each experiment follows a series of steps for training and evaluation. Initially, the multimodal model is trained. The results were replicated to the best of our ability for models proposed in previous research. The performance metrics vary according to the various tasks (binary, multi-class, multi-label, regression). For the first and second experiments, the models are evaluated under varying percentages of missing data\footnote{\textit{\textbf{Definition of missing}} We define a missing modality in relation to some percentage $p$, such that $p\%$ missing data implies that, across all observations in the dataset, approximately $p\%$ of those observations relating to a particular modality will be missing. As $p$ increases so does the number of observations set to missing. When a modality observation is marked as missing, all of its values are set to some missing value, typically 0, but for the EMT-DLFR model, the text modality is set to 100 (the <UNK> token). } (0\%-100\%) for each modality. Experiment three evaluates the two state-of-the-art models under 100\% missing modality data. The C-MAMs are then trained. Each C-MAM is comprised of a modality-specific encoder and an association network. For each model, excluding EMT-DLFR (after evaluating the baseline models for EMT-DLFR, we observed they were entirely reliant on the text modality and training a C-MAM for missing audio or video does not offer any benefit, which is an observed trait of many multimodal sentiment analysis models \cite{hazarika2022analyzing}), a C-MAM is trained for each combination of input modalities. The bi-modal tasks in experiments one and two result in two C-MAMs per baseline model. For MMIN, we train C-MAMs based on the six combinations of available modalities.
