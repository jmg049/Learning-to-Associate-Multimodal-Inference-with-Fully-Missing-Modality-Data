\subsection{Models}
This section describes the models implemented as part of each experiment. Further details, such as layer sizes and hyperparameter values, are given in Section \ref{sec:appendix} of the appendix. We define a ConvBlock as a collection of two 2D-Convolution layers with a ReLu activation function and two optional batch normalisation layers. Each multimodal model has its \textit{forward} function defined in such a way as to allow for generated features to be passed to them and skip the appropriate modality-specific sub-network of the model.

The experiments were designed to make each classification task more complex than the previous one. The purpose of each experiment is to provide experimental evidence for the hypothesis that \textbf{it is possible to learn a mapping between the input modalities} and the learnt feature space of the other modalities and that \textbf{this mapping is of sufficient quality to reduce the impact on the performance metrics} compared to when the \textbf{modality is fully missing}.

\paragraph{\textbf{Experiment 1: Binary Classification - AudioSet}} In this experiment, two multimodal models were trained to classify the audio-visual input as being recorded in an urban or rural environment (Experiment 1.1) and to classify whether or not speech was present in the recording (Experiment 1.2), respectively. While two different classification tasks were carried out using AudioSet, only one set of models was implemented. The audio model implemented consists of a single \textit{ConvBlocks} followed by a dropout and normalisation layer and three fully connected layers. A feature size of 16 is used for the audio model. Since we use ResNet50 to preprocess video inputs, the video model is relatively small, consisting of just two fully connected layers with a dropout layer and ReLU activation in between. A feature size of 256 is used for the video model. The multimodal model takes the output of the audio and video models as input and passes them to two fully connected layers connected by a dropout layer and ReLU activation.

\paragraph{\textbf{Experiment 2: Digit Classification - avMNIST}} In this experiment, we extend the classification task to multiclass classification using the avMNIST dataset. The unimodal and multimodal models were trained to classify what digit was written, spoken or both. The audio model implemented consists of two \textit{ConvBlock}, each followed by a max pooling layer. After these layers, the input is passed to two fully connected layers with a batch normalisation, dropout, and ReLU activation layers in between. A feature size of 128 is used for the audio model. The video model has two \textit{ConvBlocks}, a single max pooling layer and three fully connected layers. No dropout layers were used for the video model. A feature size of 64 is used for the video model. The multimodal model takes the output of the two unimodal models and concatenates them into a single tensor. This tensor is then passed through three fully connected layers. 

\paragraph{\textbf{Experiment 3: Human Action Classification Kinetics-Sounds}} The models trained in this experiment were designed to predict what human action, for example, playing guitar or laughing, was carried out in a given audio-visual input. The audio model used for the kinetics-sounds dataset consists of three \textit{ConvBlocks}, three average pooling layers, two dropout layers and three fully connected layers. The dropout and pooling layers are utilised between the \textit{ConvBlocks}. A feature size of 32 is used for the audio model. The implemented video model consists of three fully connected layers and a dropout layer. The input to the model is the features produced by ResNet. A feature size of 128 is used for the video model. The multimodal model comprises three fully connected layers that utilise a dropout layer.


\paragraph{\textbf{Experiment 4: Multilabel Movie Genre Classification - MM-IMDb}} In this experiment, the models were trained to predict a set of genres that were associated with a given movie based on the movie poster and a text description of the plot. The image, text, and multimodal models trained are based on those used in the paper, which first proposed the dataset \cite{arevalo2017gated}. The models are implemented as Multilayer Perceptrons (MLPs), with the multimodal model also containing a Gated Multimodal Unit (GMU) \cite{arevalo2017gated} to handle modality fusion. Each MLP has three layers: two maxout \cite{pmlr-v28-goodfellow13} linear layers and a third regular linear output layer. Batch normalisation and dropout layers are used for regularisation. The input to the image MLP is the features produced by the pre-trained VGG16 model \cite{7486599}. The text MLP uses the text representations created by a pre-trained word2vec model \cite{mikolov2013efficient}. A feature size of 512 is used for the image and text models. 
