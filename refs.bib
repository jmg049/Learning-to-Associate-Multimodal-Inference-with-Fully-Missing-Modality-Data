@string{AMSTrans = "American Mathematical Society Translations" }
@string{AMSTrans = "Amer. Math. Soc. Transl." }
@string{BullAMS = "Bulletin of the American Mathematical Society" }
@string{BullAMS = "Bull. Amer. Math. Soc." }
@string{ProcAMS = "Proceedings of the American Mathematical Society" }
@string{ProcAMS = "Proc. Amer. Math. Soc." }
@string{TransAMS = "Transactions of the American Mathematical Society" }
@string{TransAMS = "Trans. Amer. Math. Soc." }
@string{CACM = "Communications of the {ACM}" }
@string{CACM = "Commun. {ACM}" }
@string{CompServ = "Comput. Surveys" }
@string{JACM = "J. ACM" }
@string{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
@string{ACMMathSoft = "{ACM} Trans. Math. Software" }
@string{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
@string{SIGNUM = "{ACM} {SIGNUM} Newslett." }
@string{AmerSocio = "American Journal of Sociology" }
@string{AmerStatAssoc = "Journal of the American Statistical Association" }
@string{AmerStatAssoc = "J. Amer. Statist. Assoc." }
@string{ApplMathComp = "Applied Mathematics and Computation" }
@string{ApplMathComp = "Appl. Math. Comput." }
@string{AmerMathMonthly = "American Mathematical Monthly" }
@string{AmerMathMonthly = "Amer. Math. Monthly" }
@string{BIT = "{BIT}" }
@string{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
@string{BritStatPsych = "Brit. J. Math. Statist. Psych." }
@string{CanMathBull = "Canadian Mathematical Bulletin" }
@string{CanMathBull = "Canad. Math. Bull." }
@string{CompApplMath = "Journal of Computational and Applied Mathematics" }
@string{CompApplMath = "J. Comput. Appl. Math." }
@string{CompPhys = "Journal of Computational Physics" }
@string{CompPhys = "J. Comput. Phys." }
@string{CompStruct = "Computers and Structures" }
@string{CompStruct = "Comput. \& Structures" }
@string{CompJour = "The Computer Journal" }
@string{CompJour = "Comput. J." }
@string{CompSysSci = "Journal of Computer and System Sciences" }
@string{CompSysSci = "J. Comput. System Sci." }
@string{Computing = "Computing" }
@string{ContempMath = "Contemporary Mathematics" }
@string{ContempMath = "Contemp. Math." }
@string{Crelle = "Crelle's Journal" }
@string{GiornaleMath = "Giornale di Mathematiche" }
@string{GiornaleMath = "Giorn. Mat." }
@string{Computer = "{IEEE} Computer" }
@string{IEEETransComp = "{IEEE} Transactions on Computers" }
@string{IEEETransComp = "{IEEE} Trans. Comput." }
@string{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
@string{IEEETransAC = "{IEEE} Trans. Automat. Control" }
@string{IEEESpec = "{IEEE} Spectrum" }
@string{ProcIEEE = "Proceedings of the {IEEE}" }
@string{ProcIEEE = "Proc. {IEEE}" }
@string{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
@string{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }
@string{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
@string{IMANumerAna = "{IMA} J. Numer. Anal." }
@string{InfProcLet = "Information Processing Letters" }
@string{InfProcLet = "Inform. Process. Lett." }
@string{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
@string{InstMathApp = "J. Inst. Math. Appl." }
@string{IntControl = "International Journal of Control" }
@string{IntControl = "Internat. J. Control" }
@string{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
@string{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
@string{IntSuper = "International Journal of Supercomputing Applications" }
@string{IntSuper = "Internat. J. Supercomputing Applic." }
@string{Kibernetika = "Kibernetika" }
@string{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
@string{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
@string{LinAlgApp = "Linear Algebra and its Applications" }
@string{LinAlgApp = "Linear Algebra Appl." }
@string{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
@string{MathAnaAppl = "J. Math. Anal. Appl." }
@string{MathAnnalen = "Mathematische Annalen" }
@string{MathAnnalen = "Math. Ann." }
@string{MathPhys = "Journal of Mathematical Physics" }
@string{MathPhys = "J. Math. Phys." }
@string{MathComp = "Mathematics of Computation" }
@string{MathComp = "Math. Comp." }
@string{MathScand = "Mathematica Scandinavica" }
@string{MathScand = "Math. Scand." }
@string{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
@string{TablesAidsComp = "Math. Tables Aids Comput." }
@string{NumerMath = "Numerische Mathematik" }
@string{NumerMath = "Numer. Math." }
@string{PacificMath = "Pacific Journal of Mathematics" }
@string{PacificMath = "Pacific J. Math." }
@string{ParDistComp = "Journal of Parallel and Distributed Computing" }
@string{ParDistComp = "J. Parallel and Distrib. Comput." }
@string{ParComputing = "Parallel Computing" }
@string{ParComputing = "Parallel Comput." }
@string{PhilMag = "Philosophical Magazine" }
@string{PhilMag = "Philos. Mag." }
@string{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
@string{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
@string{Psychometrika = "Psychometrika" }
@string{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
@string{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
@string{QuartApplMath = "Quarterly of Applied Mathematics" }
@string{QuartApplMath = "Quart. Appl. Math." }
@string{RevueInstStat = "Review of the International Statisical Institute" }
@string{RevueInstStat = "Rev. Inst. Internat. Statist." }
@string{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
@string{JSIAM = "J. Soc. Indust. Appl. Math." }
@string{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
@string{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
@string{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
@string{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
@string{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
@string{SIAMAppMath = "{SIAM} J. Appl. Math." }
@string{SIAMComp = "{SIAM} Journal on Computing" }
@string{SIAMComp = "{SIAM} J. Comput." }
@string{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
@string{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
@string{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
@string{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
@string{SIAMReview = "{SIAM} Review" }
@string{SIAMReview = "{SIAM} Rev." }
@string{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
@string{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }
@string{SoftPracExp = "Software Practice and Experience" }
@string{SoftPracExp = "Software Prac. Experience" }
@string{StatScience = "Statistical Science" }
@string{StatScience = "Statist. Sci." }
@string{Techno = "Technometrics" }
@string{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
@string{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
@string{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
@string{VLSICompSys = "J. {VLSI} Comput. Syst." }
@string{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
@string{ZAngewMathMech = "Z. Angew. Math. Mech." }
@string{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
@string{ZAngewMathPhys = "Z. Angew. Math. Phys." }
@string{Academic = "Academic Press" }
@string{ACMPress = "{ACM} Press" }
@string{AdamHilger = "Adam Hilger" }
@string{AddisonWesley = "Addison-Wesley" }
@string{AllynBacon = "Allyn and Bacon" }
@string{AMS = "American Mathematical Society" }
@string{Birkhauser = "Birkha{\"u}ser" }
@string{CambridgePress = "Cambridge University Press" }
@string{Chelsea = "Chelsea" }
@string{ClaredonPress = "Claredon Press" }
@string{DoverPub = "Dover Publications" }
@string{Eyolles = "Eyolles" }
@string{HoltRinehartWinston = "Holt, Rinehart and Winston" }
@string{Interscience = "Interscience" }
@string{JohnsHopkinsPress = "The Johns Hopkins University Press" }
@string{JohnWileySons = "John Wiley and Sons" }
@string{Macmillan = "Macmillan" }
@string{MathWorks = "The Math Works Inc." }
@string{McGrawHill = "McGraw-Hill" }
@string{NatBurStd = "National Bureau of Standards" }
@string{NorthHolland = "North-Holland" }
@string{OxfordPress = "Oxford University Press" }
@string{PergamonPress = "Pergamon Press" }
@string{PlenumPress = "Plenum Press" }
@string{PrenticeHall = "Prentice-Hall" }
@string{SIAMPub = "{SIAM} Publications" }
@string{Springer = "Springer-Verlag" }
@string{TexasPress = "University of Texas Press" }
@string{VanNostrand = "Van Nostrand" }
@string{WHFreeman = "W. H. Freeman and Co." }
@article{smil,
	title        = {SMIL: Multimodal Learning with Severely Missing Modality},
	author       = {Ma, Mengmeng and Ren, Jian et al.},
	year         = 2021,
	month        = may,
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	number       = 3,
	pages        = {2302--2310},
	abstractnote = {A common assumption in multimodal learning is the completeness of training data, i.e., full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., ninety percent of training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks.}
}
@inproceedings{10.1145/3394486.3403234,
	title        = {Multimodal Learning with Incomplete Modalities by Knowledge Distillation},
	author       = {Wang, Qi and Zhan, Liang et al.},
	year         = 2020,
	booktitle    = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	location     = {Virtual Event, CA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Kdd '20},
	pages        = {1828–1838},
	isbn         = 9781450379984,
	abstract     = {Multimodal learning aims at utilizing information from a variety of data modalities to improve the generalization performance. One common approach is to seek the common information that is shared among different modalities for learning, whereas we can also fuse the supplementary information to leverage modality-specific information. Though the supplementary information is often desired, most existing multimodal approaches can only learn from samples with complete modalities, which wastes a considerable amount of data collected. Otherwise, model-based imputation needs to be used to complete the missing values and yet may introduce undesired noise, especially when the sample size is limited. In this paper, we proposed a framework based on knowledge distillation, utilizing the supplementary information from all modalities, and avoiding imputation and noise associated with it. Specifically, we first train models on each modality independently using all the available data. Then the trained models are used as teachers to teach the student model, which is trained with the samples having complete modalities. We demonstrate the effectiveness of the proposed method in extensive empirical studies on both synthetic datasets and real-world datasets.},
	keywords     = {multimodal learning, incomplete modalities, knowledge distillation}
}
@inproceedings{10.1145/3219819.3219963,
	title        = {Deep Adversarial Learning for Multi-Modality Missing Data Completion},
	author       = {Cai, Lei and Wang, Zhengyang et al.},
	year         = 2018,
	booktitle    = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	location     = {London, United Kingdom},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Kdd '18},
	pages        = {1158–1166},
	isbn         = 9781450355520,
	abstract     = {Multi-modality data are widely used in clinical applications, such as tumor detection and brain disease diagnosis. Different modalities can usually provide complementary information, which commonly leads to improved performance. However, some modalities are commonly missing for some subjects due to various technical and practical reasons. As a result, multi-modality data are usually incomplete, raising the multi-modality missing data completion problem. In this work, we formulate the problem as a conditional image generation task and propose an encoder-decoder deep neural network to tackle this problem. Specifically, the model takes the existing modality as input and generates the missing modality. By employing an auxiliary adversarial loss, our model is able to generate high-quality missing modality images. At the same time, we propose to incorporate the available category information of subjects in training to enable the model to generate more informative images. We evaluate our method on the Alzheimer's Disease Neuroimaging Initiative~(ADNI) database, where positron emission tomography~(PET) modalities are missing. Experimental results show that the trained network can generate high-quality PET modalities based on existing magnetic resonance imaging~(MRI) modalities, and provide complementary information to improve the detection and tracking of the Alzheimer's disease. Our results also show that the proposed methods generate higher quality images than baseline methods as measured by various image quality statistics.},
	keywords     = {disease diagnosis, adversarial loss function, deep learning, missing data completion}
}
@inproceedings{10.1145/3395035.3425202,
	title        = {Training Strategies to Handle Missing Modalities for Audio-Visual Expression Recognition},
	author       = {Parthasarathy, Srinivas and Sundaram, Shiva},
	year         = 2021,
	booktitle    = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
	location     = {Virtual Event, Netherlands},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {ICMI '20 Companion},
	pages        = {400–404},
	isbn         = 9781450380027,
	abstract     = {Automatic audio-visual expression recognition can play an important role in communication services such as tele-health, VOIP calls and human-machine interaction. Accuracy of audio-visual expression recognition could benefit from the interplay between the two modalities. However, most audio-visual expression recognition systems, trained in ideal conditions, fail to generalize in real world scenarios where either the audio or visual modality could be missing due to a number of reasons such as limited bandwidth, interactors' orientation, caller initiated muting. This paper studies the performance of a state-of-the art transformer when one of the modalities is missing. We conduct ablation studies to evaluate the model in the absence of either modality. Further, we propose a strategy to randomly ablate visual inputs during training at the clip or frame level to mimic real world scenarios. Results conducted on in-the-wild data, indicate significant generalization in proposed models trained on missing cues, with gains up to 17\% for frame level ablations, showing that these training strategies cope better with the loss of input modalities.},
	keywords     = {multimodal transformers, expression recognition, ablation training}
}
@misc{ma2021maximum,
	title        = {Maximum Likelihood Estimation for Multimodal Learning with Missing Modality},
	author       = {Fei Ma, Xiangxiang Xu et al.},
	year         = 2021,
	eprint       = {2108.10513},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{9258396,
	title        = {Deep Partial Multi-View Learning},
	author       = {Zhang, Changqing and Cui, Yajie et al.},
	year         = 2022,
	month        = may,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	number       = 5,
	pages        = {2402--2415},
	abstract     = {Although multi-view learning has made significant progress over the past few decades, it is still challenging due to the difficulty in modeling complex correlations among different views, especially under the context of view missing. To address the challenge, we propose a novel framework termed Cross Partial Multi-View Networks (CPM-Nets), which aims to fully and flexibly take advantage of multiple partial views. We first provide a formal definition of completeness and versatility for multi-view representation and then theoretically prove the versatility of the learned latent representations. For completeness, the task of learning latent multi-view representation is specifically translated to a degradation process by mimicking data transmission, such that the optimal tradeoff between consistency and complementarity across different views can be implicitly achieved. Equipped with adversarial strategy, our model stably imputes missing views, encoding information from all views for each sample to be encoded into latent representation to further enhance the completeness. Furthermore, a nonparametric classification loss is introduced to produce structured representations and prevent overfitting, which endows the algorithm with promising generalization under view-missing cases. Extensive experimental results validate the effectiveness of our algorithm over existing state of the arts for classification, representation learning and data imputation.}
}
@article{QIAN2023443,
	title        = {COM: Contrastive Masked-attention model for incomplete multimodal learning},
	author       = {Shuwei, Qian and Chongjun, Wang},
	year         = 2023,
	journal      = {Neural Networks},
	pages        = {443--455},
	keywords     = {Contrastive learning, Attention mechanism, Missing modality, Multimodal learning},
	abstract     = {Most multimodal learning methods assume that all modalities are always available in data. However, in real-world applications, the assumption is often violated due to privacy protection, sensor failure etc. Previous works for incomplete multimodal learning often suffer from one of the following drawbacks: introducing noise, lacking flexibility to missing patterns and failing to capture interactions between modalities. To overcome these challenges, we propose a COntrastive Masked-attention model (COM). The framework performs cross-modal contrastive learning with GAN-based augmentation to reduce modality gap, and employs a masked-attention model to capture interactions between modalities. The augmentation adapts cross-modal contrastive learning to suit incomplete case by a two-player game, improving the effectiveness of multimodal representations. Interactions between modalities are modeled by stacking self-attention blocks, and attention masks limit them on the observed modalities to avoid extra noise. All kinds of modality combinations share a unified architecture, so the model is flexible to different missing patterns. Extensive experiments on six datasets demonstrate the effectiveness and robustness of the proposed method for incomplete multimodal learning.}
}
@inproceedings{10.1007/978-3-031-30675-4_19,
	title        = {GRMI: Graph Representation Learning of Multimodal Data with Incompleteness},
	author       = {Xu, Xian and Xu, Xiao et al.},
	year         = 2023,
	booktitle    = {Database Systems for Advanced Applications},
	publisher    = {Springer Nature Switzerland},
	address      = {Cham},
	pages        = {286--296},
	isbn         = {978-3-031-30675-4},
	editor       = {Wang, Xin and Sapino, Maria Luisa and Han, Wook-Shin and El Abbadi, Amr and Dobbie, Gill and Feng, Zhiyong and Shao, Yingxiao and Yin, Hongzhi},
	abstract     = {Multimodal data can provide supplementary information of the subjects, which is of great potential for exploring the data-driven insights in various application scenarios. A large amount of researches focus on modal fusion to deriving quality representations of multimodal data. However, missing modality is a common issue, i.e. a sample may not contain full modalities, bringing difficulties to apply existing modal fusion methods on the incomplete multimodal data. In this paper, we present GRMI, a graph-based framework for representation learning of multimodal data with incompleteness. GRMI constructs a bipartite graph for multimodal data, where samples and modalities are viewed as two types of nodes, and the observed modality values as edges. GRMI leverages Graph Neural Network (GNN) to derive edge embeddings and sample node embeddings on the graph, which can be respectively used for missing modality imputation and modal fusion. A self-supervised strategy is utilized to pretrain the GNN by fully exploiting the multimodal data. Extensive experiment results show the superiority of the proposed framework over existing state-ofthe-art methods for both modality imputation task and modal fusion task.(The source code has been anonymously uploaded to https://github.com/GRMI2022/GRMI).}
}
@inproceedings{10.1145/3474085.3475585,
	title        = {Transformer-Based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis},
	author       = {Yuan, Ziqi and Li, Wei et al.},
	year         = 2021,
	booktitle    = {Proceedings of the 29th ACM International Conference on Multimedia},
	location     = {Virtual Event, China},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MM '21},
	pages        = {4400–4407},
	isbn         = 9781450386517,
	abstract     = {Improving robustness against data missing has become one of the core challenges in Multimodal Sentiment Analysis (MSA), which aims to judge speaker sentiments from the language, visual, and acoustic signals. In the current research, translation-based methods and tensor regularization methods are proposed for MSA with incomplete modality features. However, both of them fail to cope with random modality feature missing in non-aligned sequences. In this paper, a transformer-based feature reconstruction network (TFR-Net) is proposed to improve the robustness of models for the random missing in non-aligned modality sequences. First, intra-modal and inter-modal attention-based extractors are adopted to learn robust representations for each element in modality sequences. Then, a reconstruction module is proposed to generate the missing modality features. With the supervision of SmoothL1Loss between generated and complete sequences, TFR-Net is expected to learn semantic-level features corresponding to missing features. Extensive experiments on two public benchmark datasets show that our model achieves good results against data missing across various missing modality combinations and various missing degrees.},
	keywords     = {multimodal sentiment analysis, transformer, data missing, feature reconstruction}
}
@inproceedings{10.1145/3240508.3240528,
	title        = {Semi-Supervised Deep Generative Modelling of Incomplete Multi-Modality Emotional Data},
	author       = {Du, Changde and Du, Changying et al.},
	year         = 2018,
	booktitle    = {Proceedings of the 26th ACM International Conference on Multimedia},
	location     = {Seoul, Republic of Korea},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MM '18},
	pages        = {108–116},
	isbn         = 9781450356657,
	abstract     = {There are threefold challenges in emotion recognition. First, it is difficult to recognize human's emotional states only considering a single modality. Second, it is expensive to manually annotate the emotional data. Third, emotional data often suffers from missing modalities due to unforeseeable sensor malfunction or configuration issues. In this paper, we address all these problems under a novel multi-view deep generative framework. Specifically, we propose to model the statistical relationships of multi-modality emotional data using multiple modality-specific generative networks with a shared latent space. By imposing a Gaussian mixture assumption on the posterior approximation of the shared latent variables, our framework can learn the joint deep representation from multiple modalities and evaluate the importance of each modality simultaneously. To solve the labeled-data-scarcity problem, we extend our multi-view model to semi-supervised learning scenario by casting the semi-supervised classification problem as a specialized missing data imputation task. To address the missing-modality problem, we further extend our semi-supervised multi-view model to deal with incomplete data, where a missing view is treated as a latent variable and integrated out during inference. This way, the proposed overall framework can utilize all available (both labeled and unlabeled, as well as both complete and incomplete) data to improve its generalization ability. The experiments conducted on two real multi-modal emotion datasets demonstrated the superiority of our framework.},
	keywords     = {multi-view semi-supervised learning, multi-modal emotion recognition, deep generative model, incomplete data}
}
@article{YUAN2012622,
	title        = {Multi-source feature learning for joint analysis of incomplete multiple heterogeneous neuroimaging data},
	author       = {Lei Yuan and Yalin Wang et al.},
	year         = 2012,
	journal      = {NeuroImage},
	number       = 3,
	pages        = {622--632},
	keywords     = {Multi-source feature learning, Multi-task learning, Incomplete data, Ensemble},
	abstract     = {Analysis of incomplete data is a big challenge when integrating large-scale brain imaging datasets from different imaging modalities. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), for example, over half of the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing measures are discarded, resulting in a severe loss of available information. In this paper, we address this problem by proposing an incomplete Multi-Source Feature (iMSF) learning method where all the samples (with at least one available data source) can be used. To illustrate the proposed approach, we classify patients from the ADNI study into groups with Alzheimer's disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI's 780 participants (172AD, 397 MCI, 211 NC), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithm. Depending on the problem being solved, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. To build a practical and robust system, we construct a classifier ensemble by combining our method with four other methods for missing value estimation. Comprehensive experiments with various parameters show that our proposed iMSF method and the ensemble model yield stable and promising results.}
}
@article{9099281,
	title        = {Multi-Modal Stacked Denoising Autoencoder for Handling Missing Data in Healthcare Big Data},
	author       = {Kim, Joo-Chang and Chung, Kyungyong},
	year         = 2020,
	journal      = {IEEE Access},
	pages        = {104933--104943},
	abstract     = {Supply and demand increase in response to healthcare trends. Moreover, personal health records (PHRs) are being managed by individuals. Such records are collected using different avenues and vary considerably in terms of their type and scope depending on the particular circumstances. As a result, some data may be missing, which has a negative effect on the data analysis, and such data should, therefore, be replaced with appropriate values. In this study, a method for estimating missing data using a multi-modal autoencoder applied to the field of healthcare big data is proposed. The proposed method uses a stacked denoising autoencoder to estimate the missing data that occur during the data collection and processing stages. Autoencoders are neural networks that output value of x^ similar to an input value of x. In the present study, data from the Korean National Health Nutrition Examination Survey (KNHNES), conducted by the Korea Centers for Disease Control and Prevention (KCDC), are used. As representative healthcare data from South Korea, they contain a large number of parameters identical to those used in the PHRs. Based on this, models can be generated to estimate missing data occurring in PHRs. Furthermore, PHRs involve a multi-modality that allows the data to be collected from multiple sources for a single object. Therefore, the stacked denoising autoencoder applied is configured under a multi-modal setting. Through pre-processing, a set of data without missing value in KNHNES is designed. In the data set based learning, a label is set as original data, and an autoencoder input is set as noised input that additionally has as many random zero numbers as noise factor. In this way, the autoencoder learns in the way of making the zero-based noise value similar to the original label value. When the amount of missing data in a dataset reaches approximately 25%, the accuracy of the proposed method using a multi-modal stacked denoising autoencoder is 0.9217, which is higher than that achieved by other ordinary methods. For a single-modal denoising autoencoder, the accuracy is 0.932, with a slight difference of approximately 0.01, which falls within the allowable limits in data analysis. In terms of computational performance, a single-modal autoencoder has 10,384 parameters, which is 5,594 more than those used in a multi-modal stacked autoencoder. These parameters affect the speed of the model. Both models exhibit a significant difference in the number of parameters but demonstrate a relatively small difference in accuracy, suggesting that the proposed multi-modal stacked denoising autoencoder is advantageous over a single-modal model when used on a personal device. Moreover, a multi-modal model can save additional time when processing large amounts of data in locations such as hospitals and institutions.}
}
@inproceedings{zhao-etal-2021-missing,
	title        = {Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities},
	author       = {Zhao, Jinming and Li, Ruichen  et al.},
	year         = 2021,
	month        = aug,
	booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Online},
	pages        = {2608--2618},
	abstract     = {Multimodal fusion has been proved to improve emotion recognition performance in previous works. However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain. It makes the fixed multimodal fusion fail in such cases. In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem. MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition. The code will be available at https://github.com/AIM3-RUC/MMIN.}
}
@article{9755996,
	title        = {Unsupervised Multimodal Anomaly Detection With Missing Sources for Liquid Rocket Engine},
	author       = {Feng, Yong and Liu, Zijun et al.},
	year         = 2022,
	journal      = {IEEE Transactions on Neural Networks and Learning Systems},
	pages        = {1--15},
	abstract     = {To achieve reliable and automatic anomaly detection (AD) for large equipment such as liquid rocket engine (LRE), multisource data are commonly manipulated in deep learning pipelines. However, current AD methods mainly aim at single source or single modality, whereas existing multimodal methods cannot effectively cope with a common issue, modality incompleteness. To this end, we propose an unsupervised multimodal method for AD with missing sources in LRE system. The proposed method handles intramodality fusion, intermodality fusion, and decision fusion in a unified framework composed of multiple deep autoencoders (AEs) and a skip-connected AE. Specifically, the first module restores missing sources to construct a complete modality, thus advancing the secondary reconstruction. Different from vanilla reconstruction-based methods, the proposed method minimizes reconstruction loss and meanwhile maximizes the dissimilarity of representations in two latent spaces. Utilizing reconstruction errors and latent representation discrepancy, the anomaly score is acquired. At decision level, the model performance can be further enhanced via anomaly score fusion. To demonstrate the effectiveness, extensive experiments are carried out on multivariate time-series data from static ignition of several LREs. The results indicate the superiority and potential of the proposed method for AD with missing sources for LRE.}
}
@inproceedings{Tran_2017_CVPR,
	title        = {Missing Modalities Imputation via Cascaded Residual Autoencoder},
	author       = {Tran, Luan and Liu, Xiaoming et al.},
	year         = 2017,
	month        = jul,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@inproceedings{Matsuura_2018_ECCV_Workshops,
	title        = {Generalized Bayesian Canonical Correlation Analysis with Missing Modalities},
	author       = {Matsuura, Toshihiko and Saito, Kuniaki et al.},
	year         = 2018,
	month        = sep,
	booktitle    = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops}
}
@inproceedings{pmlr-v37-wangb15,
	title        = {On Deep Multi-View Representation Learning},
	author       = {Wang, Weiran and Arora, Raman et al.},
	year         = 2015,
	month        = {07--09 Jul},
	booktitle    = {Proceedings of the 32nd International Conference on Machine Learning},
	publisher    = {Pmlr},
	address      = {Lille, France},
	series       = {Proceedings of Machine Learning Research},
	pages        = {1083--1092},
	editor       = {Bach, Francis and Blei, David},
	pdf          = {http://proceedings.mlr.press/v37/wangb15.pdf},
	abstract     = {We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).}
}
@inproceedings{pmlr-v28-andrew13,
	title        = {Deep Canonical Correlation Analysis},
	author       = {Andrew, Galen and Arora, Raman et al.},
	year         = 2013,
	month        = {17--19 Jun},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {Pmlr},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	number       = 3,
	pages        = {1247--1255},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/andrew13.pdf},
	abstract     = {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}
@article{5466511,
	title        = {Matrix Completion From a Few Entries},
	author       = {Keshavan, Raghunandan H. and Montanari, Andrea et al.},
	year         = 2010,
	month        = jun,
	journal      = {IEEE Transactions on Information Theory},
	number       = 6,
	pages        = {2980--2998},
	abstract     = {Let M be an n¿ × n matrix of rank r, and assume that a uniformly random subset E of its entries is observed. We describe an efficient algorithm, which we call OptSpace, that reconstructs M from |E| = O(rn) observed entries with relative root mean square error 1/2 RMSE ¿ C(¿) (nr/|E|)1/2 with probability larger than 1 - 1/n3. Further, if r = O(1) and M is sufficiently unstructured, then OptSpace reconstructs it exactly from |E| = O(n log n) entries with probability larger than 1 - 1/n3. This settles (in the case of bounded rank) a question left open by Candes and Recht and improves over the guarantees for their reconstruction algorithm. The complexity of our algorithm is O(|E|r log n), which opens the way to its use for massive data sets. In the process of proving these statements, we obtain a generalization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek on the spectrum of sparse random matrices.}
}
@article{mazumder2010spectral,
	title        = {Spectral regularization algorithms for learning large incomplete matrices},
	author       = {Mazumder, Rahul and Hastie, Trevor et al.},
	year         = 2010,
	journal      = {The Journal of Machine Learning Research},
	publisher    = {JMLR. org},
	pages        = {2287--2322}
}
@misc{zadeh2021variational,
	title        = {Variational Auto-Decoder: A Method for Neural Generative Modeling from Incomplete Data},
	author       = {Amir Zadeh and Yao-Chong Lim et al.},
	year         = 2021,
	eprint       = {1903.00840},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{wang2020makes,
	title        = {What makes training multi-modal classification networks hard?},
	author       = {Wang, Weiyao and Tran, Du et al.},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {12695--12705}
}
@inproceedings{Peng2022Balanced,
	title        = {Balanced Multimodal Learning via On-the-fly Gradient Modulation},
	author       = {Peng, Xiaokang and Wei, Yake et al.},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}
}
@inproceedings{10096655,
	title        = {MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning},
	author       = {Xu, Ruize and Feng, Ruoxuan et al.},
	year         = 2023,
	month        = jun,
	booktitle    = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1--5},
	abstract     = {Audio-visual learning helps to comprehensively understand the world by fusing practical information from multiple modalities. However, recent studies show that the imbalanced optimization of uni-modal encoders in a joint-learning model is a bottleneck to enhancing the model’s performance. We further find that the up-to-date imbalance-mitigating methods fail on some audio-visual fine-grained tasks, which have a higher demand for distinguishable feature distribution. Fueled by the success of cosine loss that builds hyperspherical feature spaces and achieves lower intra-class angular variability, this paper proposes Multi-Modal Cosine loss, MMCosine. It performs a modality-wise L2 normalization to features and weights towards balanced and better multi-modal fine-grained learning. We demonstrate that our method can alleviate the imbalanced optimization from the perspective of weight norm and fully exploit the discriminability of the cosine metric. Extensive experiments prove the effectiveness of our method and the versatility with advanced multi-modal fusion strategies and up-to-date imbalance-mitigating methods. The project page is https://gewu-lab.github.io/MMCosine/.}
}
@inproceedings{DBLP:journals/corr/abs-1806-06176,
	title        = {Learning Factorized Multimodal Representations},
	author       = {Yao{-}Hung Hubert Tsai and Paul Pu Liang et al.},
	year         = 2019,
	booktitle    = {Iclr}
}
@article{10.14778/2732296.2732301,
	title        = {Effective Multi-Modal Retrieval Based on Stacked Auto-Encoders},
	author       = {Wang, Wei and Ooi, Beng Chin et al.},
	year         = 2014,
	month        = apr,
	journal      = {Proc. VLDB Endow.},
	publisher    = {VLDB Endowment},
	number       = 8,
	pages        = {649–660},
	issue_date   = {April 2014},
	abstract     = {Multi-modal retrieval is emerging as a new search paradigm that enables seamless information retrieval from various types of media. For example, users can simply snap a movie poster to search relevant reviews and trailers. To solve the problem, a set of mapping functions are learned to project high-dimensional features extracted from data of different media types into a common low-dimensional space so that metric distance measures can be applied. In this paper, we propose an effective mapping mechanism based on deep learning (i.e., stacked auto-encoders) for multi-modal retrieval. Mapping functions are learned by optimizing a new objective function, which captures both intra-modal and inter-modal semantic relationships of data from heterogeneous sources effectively. Compared with previous works which require a substantial amount of prior knowledge such as similarity matrices of intra-modal data and ranking examples, our method requires little prior knowledge. Given a large training dataset, we split it into mini-batches and continually adjust the mapping functions for each batch of input. Hence, our method is memory efficient with respect to the data volume. Experiments on three real datasets illustrate that our proposed method achieves significant improvement in search accuracy over the state-of-the-art methods.},
}
@misc{zadeh2016mosi,
	title        = {MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos},
	author       = {Amir Zadeh and Rowan Zellers et al.},
	year         = 2016,
	eprint       = {1606.06259},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@inproceedings{10.5555/3104482.3104569,
	title        = {Multimodal Deep Learning},
	author       = {Ngiam, Jiquan and Khosla, Aditya et al.},
	year         = 2011,
	booktitle    = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	location     = {Bellevue, Washington, USA},
	publisher    = {Omnipress},
	address      = {Madison, WI, USA},
	series       = {Icml'11},
	pages        = {689–696},
	isbn         = 9781450306195,
	abstract     = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.},
}
@inproceedings{10.1145/3394486.3403182,
	title        = {HGMF: Heterogeneous Graph-Based Fusion for Multimodal Data with Incompleteness},
	author       = {Chen, Jiayi and Zhang, Aidong},
	year         = 2020,
	booktitle    = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	location     = {Virtual Event, CA, USA},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {Kdd '20},
	pages        = {1295–1305},
	isbn         = 9781450379984,
	abstract     = {With the advances in data collection techniques, large amounts of multimodal data collected from multiple sources are becoming available. Such multimodal data can provide complementary information that can reveal fundamental characteristics of real-world subjects. Thus, multimodal machine learning has become an active research area. Extensive works have been developed to exploit multimodal interactions and integrate multi-source information. However, multimodal data in the real world usually comes with missing modalities due to various reasons, such as sensor damage, data corruption, and human mistakes in recording. Effectively integrating and analyzing multimodal data with incompleteness remains a challenging problem. We propose a Heterogeneous Graph-based Multimodal Fusion (HGMF) approach to enable multimodal fusion of incomplete data within a heterogeneous graph structure. The proposed approach develops a unique strategy for learning on incomplete multimodal data without data deletion or data imputation. More specifically, we construct a heterogeneous hypernode graph to model the multimodal data having different combinations of missing modalities, and then we formulate a graph neural network based transductive learning framework to project the heterogeneous incomplete data onto a unified embedding space, and multi-modalities are fused along the way. The learning framework captures modality interactions from available data, and leverages the relationships between different incompleteness patterns. Our experimental results demonstrate that the proposed method outperforms existing graph-based as well as non-graph based baselines on three different datasets.},
	keywords     = {heterogeneous graph, missing modalities, data incompleteness, graph neural networks, multimodal fusion}
}
@article{8253467,
	title        = {Multi-View Missing Data Completion},
	author       = {Zhang, Lei and Zhao, Yao et al.},
	year         = 2018,
	month        = jul,
	journal      = {IEEE Transactions on Knowledge and Data Engineering},
	number       = 7,
	pages        = {1296--1309},
	abstract     = {A growing number of multi-view data arises naturally in many scenarios, including medical diagnosis, webpage classification, and multimedia analysis. A challenge in learning from multi-view data is that not all instances are fully represented in all views, resulting in missing view data. In this paper, we focus on feature-level completion for missing view of multi-view data. Aiming at capturing both semantic complementarity and identical distribution among different views, an Isomorphic Linear Correlation Analysis (ILCA) method is proposed to linearly map multi-view data to a feature-isomorphic subspace through learning a set of excellent isomorphic features, thereby unfolding the shared information from different views. Meanwhile, we assume that missing view obeys normal distribution. Then, the missing view data matrix can be modeled as a low-rank component plus a sparse contribution. Thus, to accomplish missing view completion, an Identical Distribution Pursuit Completion (IDPC) model based on the learned features is proposed, in which the identical distribution constraint of missing view to the other available one in the feature-isomorphic subspace is fully exploited. Comprehensive experiments on several multi-view datasets demonstrate that our proposed framework yields promising results.}
}
@article{7993002,
	title        = {Multi-Hypergraph Learning for Incomplete Multimodality Data},
	author       = {Liu, Mingxia and Gao, Yue et al.},
	year         = 2018,
	month        = jul,
	journal      = {IEEE Journal of Biomedical and Health Informatics},
	number       = 4,
	pages        = {1197--1208},
	abstract     = {Multi-modality data convey complementary information that can be used to improve the accuracy of prediction models in disease diagnosis. However, effectively integrating multi-modality data remains a challenging problem, especially when the data are incomplete. For instance, more than half of the subjects in the Alzheimer's disease neuroimaging initiative (ADNI) database have no fluorodeoxyglucose positron emission tomography and cerebrospinal fluid data. Currently, there are two commonly used strategies to handle the problem of incomplete data: 1) discard samples having missing features; and 2) impute those missing values via specific techniques. In the first case, a significant amount of useful information is lost and, in the second case, additional noise and artifacts might be introduced into the data. Also, previous studies generally focus on the pairwise relationships among subjects, without considering their underlying complex (e.g., high-order) relationships. To address these issues, in this paper, we propose a multi-hypergraph learning method for dealing with incomplete multimodality data. Specifically, we first construct multiple hypergraphs to represent the high-order relationships among subjects by dividing them into several groups according to the availability of their data modalities. A hypergraph regularized transductive learning method is then applied to these groups for automatic diagnosis of brain diseases. Extensive evaluation of the proposed method using all subjects in the baseline ADNI database indicates that our method achieves promising results in AD/MCI classification, compared with the state-of-the-art methods.}
}
@inproceedings{audioset,
	title        = {Audio Set: An ontology and human-labeled dataset for audio events},
	author       = {Jort F. Gemmeke and Daniel P. W. Ellis et al.},
	year         = 2017,
	booktitle    = {Proc. IEEE ICASSP 2017},
	address      = {New Orleans, LA}
}
@misc{tvqa,
	title        = {On Modality Bias in the TVQA Dataset},
	author       = {Thomas Winterbottom and Sarah Xiao et al.},
	year         = 2020,
	eprint       = {2012.10210},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{kinetics,
	title        = {The Kinetics Human Action Video Dataset},
	author       = {Will Kay and Joao Carreira et al.},
	year         = 2017,
	eprint       = {1705.06950},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{kinetics_sounds,
	title        = {Look, Listen and Learn},
	author       = {Arandjelovic, Relja and Zisserman, Andrew},
	year         = 2017,
	month        = oct,
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@article{becker2018interpreting,
	title        = {Interpreting and Explaining Deep Neural Networks for Classification of Audio Signals},
	author       = {Becker, S\"oren and Ackermann, Marcel et al.},
	year         = 2018,
	journal      = {CoRR},
	archiveprefix = {arXiv},
	eprint       = {1807.03418}
}
@misc{vielzeuf2018centralnet,
	title        = {CentralNet: a Multilayer Approach for Multimodal Fusion},
	author       = {Valentin Vielzeuf and Alexis Lechervy et al.},
	year         = 2018,
	eprint       = {1808.07275},
	archiveprefix = {arXiv},
	primaryclass = {cs.AI}
}
@misc{arevalo2017gated,
	title        = {Gated Multimodal Units for Information Fusion},
	author       = {John Arevalo and Thamar Solorio et al.},
	year         = 2017,
	eprint       = {1702.01992},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@inproceedings{8273601,
	title        = {Multimodal autoencoder: A deep learning approach to filling in missing sensor data and enabling better mood prediction},
	author       = {Jaques, Natasha and Taylor, Sara et al.},
	year         = 2017,
	month        = oct,
	booktitle    = {2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)},
	pages        = {202--208},
	abstract     = {To accomplish forecasting of mood in real-world situations, affective computing systems need to collect and learn from multimodal data collected over weeks or months of daily use. Such systems are likely to encounter frequent data loss, e.g. when a phone loses location access, or when a sensor is recharging. Lost data can handicap classifiers trained with all modalities present in the data. This paper describes a new technique for handling missing multimodal data using a specialized denoising autoencoder: the Multimodal Autoencoder (MMAE). Empirical results from over 200 participants and 5500 days of data demonstrate that the MMAE is able to predict the feature values from multiple missing modalities more accurately than reconstruction methods such as principal components analysis (PCA). We discuss several practical benefits of the MMAE's encoding and show that it can provide robust mood prediction even when up to three quarters of the data sources are lost.}
}
@inproceedings{NIPS2012_6cdd60ea,
	title        = {Image Denoising and Inpainting with Deep Neural Networks},
	author       = {Xie, Junyuan and Xu, Linli et al.},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	editor       = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger}
}
@inproceedings{10.1007/978-3-030-03493-1_62,
	title        = {Signal Reconstruction Using Evolvable Recurrent Neural Networks},
	author       = {Khan, Nadia Masood and Khan, Gul Muhammad},
	year         = 2018,
	booktitle    = {Intelligent Data Engineering and Automated Learning -- IDEAL 2018},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {594--602},
	isbn         = {978-3-030-03493-1},
	editor       = {Yin, Hujun and Camacho, David and Novais, Paulo and Tall{\'o}n-Ballesteros, Antonio J.},
	abstract     = {Accurate reconstruction of under-sampled data plays an important role in wireless transmission of signals. A novel approach to reconstruct randomly missing data based on interpolation and machine learning technique i.e. Cartesian genetic programming evolved recurrent neural network (CGPRNN) is proposed in this research. Although feed-forward Neural networks have been very successful in signal processing fields in general with recurrent neural networks having an edge where system with memory is priority. Recurrent neural networks not only provide non-linearity but also non-Markovian state information. The proposed method is used for reconstruction of lost samples in audio signal which are non-stationary in nature through accurate predication. Simulation results are presented to validate the performance of CGPRNN for accurate reconstruction of distorted signal. The error rate of 12{\%} for 25{\%} missing data and 18{\%} for 50{\%} distorted data has been achieved where the system has low confidence in its predication.}
}
@inproceedings{pmlr-v28-goodfellow13,
	title        = {Maxout Networks},
	author       = {Goodfellow, Ian and Warde-Farley, David et al.},
	year         = 2013,
	month        = {17--19 Jun},
	booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
	publisher    = {Pmlr},
	address      = {Atlanta, Georgia, USA},
	series       = {Proceedings of Machine Learning Research},
	number       = 3,
	pages        = {1319--1327},
	editor       = {Dasgupta, Sanjoy and McAllester, David},
	pdf          = {http://proceedings.mlr.press/v28/goodfellow13.pdf},
	abstract     = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.}
}
@inproceedings{7486599,
	title        = {Very deep convolutional neural network based image classification using small training sample size},
	author       = {Liu, Shuying and Deng, Weihong},
	year         = 2015,
	month        = nov,
	booktitle    = {2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)},
	pages        = {730--734},
	abstract     = {Since Krizhevsky won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 competition with the brilliant deep convolutional neural networks (D-CNNs), researchers have designed lots of D-CNNs. However, almost all the existing very deep convolutional neural networks are trained on the giant ImageNet datasets. Small datasets like CIFAR-10 has rarely taken advantage of the power of depth since deep models are easy to overfit. In this paper, we proposed a modified VGG-16 network and used this model to fit CIFAR-10. By adding stronger regularizer and using Batch Normalization, we achieved 8.45% error rate on CIFAR-10 without severe overfitting. Our results show that the very deep CNN can be used to fit small datasets with simple and proper modifications and don't need to re-design specific small networks. We believe that if a model is strong enough to fit a large dataset, it can also fit a small one.}
}
@misc{mikolov2013efficient,
	title        = {Efficient Estimation of Word Representations in Vector Space},
	author       = {Tomas Mikolov and Kai Chen et al.},
	year         = 2013,
	eprint       = {1301.3781},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{he2015deep,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {Kaiming He and Xiangyu Zhang et al.},
	year         = 2015,
	eprint       = {1512.03385},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@incollection{45611,
	title        = {CNN Architectures for Large-Scale Audio Classification},
	author       = {Shawn Hershey and Sourish Chaudhuri et al.},
	year         = 2017,
	booktitle    = {International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}
@article{MADJAROV20123084,
	title        = {An extensive experimental comparison of methods for multi-label learning},
	author       = {Gjorgji Madjarov and Dragi Kocev et al.},
	year         = 2012,
	journal      = {Pattern Recognition},
	number       = 9,
	pages        = {3084--3104},
	note         = {Best Papers of Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA'2011)},
	keywords     = {Multi-label ranking, Multi-label classification, Comparison of multi-label learning methods},
	abstract     = {Multi-label learning has received significant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efficiency in terms of time needed to learn a classifier and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical significance of differences in performance. The results of the analysis show that for multi-label classification the best performing methods overall are random forests of predictive clustering trees (RF-PCT) and hierarchy of multi-label classifiers (HOMER), followed by binary relevance (BR) and classifier chains (CC). Furthermore, RF-PCT exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to RF-PCT and HOMER using multiple evaluation measures.}
}
@inproceedings{7965870,
	title        = {Variational methods for conditional multimodal deep learning},
	author       = {Pandey, Gaurav and Dukkipati, Ambedkar},
	year         = 2017,
	month        = may,
	booktitle    = {2017 International Joint Conference on Neural Networks (IJCNN)},
	pages        = {308--315},
	abstract     = {In this paper, we address the problem of conditional modality learning, whereby one is interested in generating one modality given the other. While it is straightforward to learn a joint distribution over multiple modalities using a deep multi-modal architecture, we observe that such models are not very effective at conditional generation. Hence, we address the problem by learning conditional distributions between the modalities. We use variational methods for maximizing the corresponding conditional log-likelihood. The resultant deep model, which we refer to as conditional multimodal autoencoder (CMMA), forces the latent representation obtained from a single modality alone to be `close' to the joint representation obtained from multiple modalities. We use the proposed model to generate faces from attributes. We show that the faces generated from attributes using the proposed model are qualitatively and quantitatively more representative of the attributes from which they were generated, than those obtained by other deep generative models. We also propose a secondary task, whereby the existing faces are modified by modifying the corresponding attributes. We observe that the modifications in face introduced by the proposed model are representative of the corresponding modifications in attributes.}
}
@inproceedings{NIPS2015_8d55a249,
	title        = {Learning Structured Output Representation using Deep Conditional Generative Models},
	author       = {Sohn, Kihyuk and Lee, Honglak et al.},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	editor       = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett}
}
@article{10122560,
	title        = {Efficient Multimodal Transformer With Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis},
	author       = {Sun, Licai and Lian, Zheng et al.},
	year         = 2024,
	journal      = {IEEE Transactions on Affective Computing},
	number       = 1,
	pages        = {309--325},
	keywords     = {Transformers;Robustness;Semantics;Data models;Computational modeling;Videos;Training;Multimodal sentiment analysis;unaligned and incomplete data;efficient multimodal Transformer;dual-level feature restoration;robustness}
}
@article{mosi,
	title        = {{MOSI:} Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos},
	author       = {Amir Zadeh and Rowan Zellers et al.},
	year         = 2016,
	journal      = {CoRR},
	eprinttype   = {arXiv},
	eprint       = {1606.06259},
	timestamp    = {Thu, 25 Jul 2019 10:47:43 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/ZadehZPM16.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{mosei,
	title        = {Multimodal Language Analysis in the Wild: {CMU}-{MOSEI} Dataset and Interpretable Dynamic Fusion Graph},
	author       = {Bagher Zadeh, AmirAli and Liang, Paul Pu  et al.},
	year         = 2018,
	month        = jul,
	booktitle    = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher    = {Association for Computational Linguistics},
	address      = {Melbourne, Australia},
	pages        = {2236--2246},
	editor       = {Gurevych, Iryna  and Miyao, Yusuke},
	abstract     = {Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.}
}
@article{hazarika2022analyzing,
	title        = {Analyzing Modality Robustness in Multimodal Sentiment Analysis},
	author       = {Hazarika, Devamanyu and Li, Yingting et al.},
	year         = 2022,
	publisher    = {Naacl}
}
@article{geraghty2023understanding,
	title        = {Understanding the Relevancy of Modality Information in Multimodal Machine Learning},
	author       = {Geraghty, Jack and Hines, Andrew and Golpayegani, Fatemeh},
	year         = 2023,
	journal      = {Modelling and Representing Context (MRC), European Conference on Artificial Intelligence (ECAI)},
	publisher    = {Modelling and Representing Context (MRC) @ ECAI}
}
@inproceedings{NEURIPS2020_20d749bc,
	title        = {Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies},
	author       = {Gat, Itai and Schwartz, Idan et al.},
	year         = 2020,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	pages        = {3197--3208},
	editor       = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{busso2008iemocap,
	title        = {IEMOCAP: Interactive emotional dyadic motion capture database},
	author       = {Busso, Carlos and Bulut, Murtaza et al.},
	year         = 2008,
	journal      = {Language resources and evaluation},
	publisher    = {Springer},
	pages        = {335--359}
}
@article{xu2019learning,
	title        = {Learning alignment for multimodal emotion recognition from speech},
	author       = {Xu, Haiyang and Zhang, Hui et al.},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.05645}
}
@inproceedings{liang2020semi,
	title        = {Semi-supervised multi-modal emotion recognition with cross-modal distribution matching},
	author       = {Liang, Jingjun and Li, Ruichen et al.},
	year         = 2020,
	booktitle    = {Proceedings of the 28th ACM international conference on multimedia},
	pages        = {2852--2861}
}
@article{Busso_2017,
	title        = {{MSP-IMPROV}: An Acted Corpus of Dyadic Interactions to Study Emotion Perception},
	author       = {C. Busso and S. Parthasarathy et al.},
	year         = 2017,
	month        = {January-March},
	journal      = {IEEE Transactions on Affective Computing},
	number       = 1,
	pages        = {67--80}
}
@inproceedings{zhu2017unpaired,
	title        = {Unpaired image-to-image translation using cycle-consistent adversarial networks},
	author       = {Zhu, Jun-Yan and Park, Taesung et al.},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE international conference on computer vision},
	pages        = {2223--2232}
}
@article{9563268,
	title        = {Audio-Driven Talking Video Frame Restoration},
	author       = {Cheng, Harry and Guo, Yangyang et al.},
	year         = 2024,
	journal      = {IEEE Transactions on Multimedia},
	pages        = {4110--4122},
	abstract     = {Talking video frames occasionally drop while streaming for reasons like network errors, which greatly hurts the online team collaboration and user experiences. Directly generating the dropped frames from the remaining ones is unfavorable since a person’s lip motion is usually non-linear and thus hard to be restored when consecutive frames are missing. Nevertheless, the audio content provides strong signals for lip motion and is less likely to drop during transmitting. Inspired by this, as an initial attempt, we present the task of audio-driven talking video frame restoration in this paper, i.e., restoring dropped video frames by jointly leveraging the audio and remaining video frames. Towards the high-quality frame generation, we devise a cross-modal frame restoration network. This network aligns the complete audio content with video frames, precisely identifies and sequentially generates the dropped frames. To justify our model, we construct a new dataset, Talking Video Frames Drop, TVFD for short, consisting of 2.5K video and 144K frames in total. We conduct extensive experiments over TVFD and another publicly accessible dataset - Voxceleb2. Our model obtains significantly improved performance as compared to other state-of-the-art competitors.},
	keywords     = {Streaming media;Faces;Lips;Task analysis;Image restoration;Visualization;Synchronization;Frame Restoration;Frame-Dropped Video;Cross-Modal Learning;Dynamic Programming;Generative Adversial Network}
}
@article{9842374,
	title        = {Perception-Aware Cross-Modal Signal Reconstruction: From Audio-Haptic to Visual},
	author       = {Wei, Xin and Yao, Yuyuan et al.},
	year         = 2023,
	journal      = {IEEE Transactions on Multimedia},
	pages        = {5527--5538},
	keywords     = {Visualization;Haptic interfaces;Redundancy;Signal reconstruction;Signal restoration;Correlation;Streaming media;Audio-haptic redundancy elimination;cross-modal communications;perception;visual signal reconstruction}
}
@article{seeing_what_you_hear,
	title        = {Seeing What You Hear: Cross-Modal Illusions and Perception},
	author       = {Casey O'Callaghan},
	year         = 2008,
	journal      = {Philosophical Issues},
	publisher    = {[Wiley, Ridgeview Publishing Company]},
	pages        = {316--338},
	urldate      = {2024-06-07}
}
@article{LEHMANN2005326,
	title        = {The role of multisensory memories in unisensory object discrimination},
	author       = {Sandra Lehmann and Micah M. Murray},
	year         = 2005,
	journal      = {Cognitive Brain Research},
	number       = 2,
	pages        = {326--334},
	keywords     = {Multisensory, Auditory, Visual, Somatosensory, Memory, Object recognition, Discrimination},
	abstract     = {Past multisensory experiences can influence current unisensory processing and memory performance. Repeated images are better discriminated if initially presented as auditory–visual pairs, rather than only visually. An experience's context thus plays a role in how well repetitions of certain aspects are later recognized. Here, we investigated factors during the initial multisensory experience that are essential for generating improved memory performance. Subjects discriminated repeated versus initial image presentations intermixed within a continuous recognition task. Half of initial presentations were multisensory, and all repetitions were only visual. Experiment 1 examined whether purely episodic multisensory information suffices for enhancing later discrimination performance by pairing visual objects with either tones or vibrations. We could therefore also assess whether effects can be elicited with different sensory pairings. Experiment 2 examined semantic context by manipulating the congruence between auditory and visual object stimuli within blocks of trials. Relative to images only encountered visually, accuracy in discriminating image repetitions was significantly impaired by auditory–visual, yet unaffected by somatosensory–visual multisensory memory traces. By contrast, this accuracy was selectively enhanced for visual stimuli with semantically congruent multisensory pasts and unchanged for those with semantically incongruent multisensory pasts. The collective results reveal opposing effects of purely episodic versus semantic information from auditory–visual multisensory events. Nonetheless, both types of multisensory memory traces are accessible for processing incoming stimuli and indeed result in distinct visual object processing, leading to either impaired or enhanced performance relative to unisensory memory traces. We discuss these results as supporting a model of object-based multisensory interactions.}
}
@article{Glicksohn2013,
	title        = {The role of cross-modal associations in statistical learning},
	author       = {Glicksohn, Arit and Cohen, Asher},
	year         = 2013,
	month        = dec,
	day          = {01},
	journal      = {Psychonomic Bulletin {\&} Review},
	number       = 6,
	pages        = {1161--1169},
	abstract     = {Our environment is richly structured, with objects producing correlated information within and across sensory modalities. A prominent challenge faced by our perceptual system is to learn such regularities. Here, we examined statistical learning and addressed learners' ability to track transitional probabilities between elements in the auditory and visual modalities. Specifically, we investigated whether cross-modal information affects statistical learning within a single modality. Participants were familiarized with a statistically structured modality (e.g., either audition or vision) accompanied by different types of cues in a second modality (e.g., vision or audition). The results revealed that statistical learning within either modality is affected by cross-modal information, with learning being enhanced or reduced according to the type of cue provided in the second modality.}
}
@article{Mondloch2004,
	title        = {Do small white balls squeak? Pitch-object correspondences in young children},
	author       = {Mondloch, Catherine J. and Maurer, Daphne},
	year         = 2004,
	month        = jun,
	day          = {01},
	journal      = {Cognitive, Affective, {\&} Behavioral Neuroscience},
	number       = 2,
	pages        = {133--136},
	abstract     = {Adults with auditory-visual synesthesia agree that higher pitched sounds induce smaller, brighter visual percepts. We have hypothesized that these correspondences are remnants of cross-modal neural connections that are present at birth and that influence the development of perception and language even in adults and children without synesthesia. In this study, we explored these correspondences in preschoolers (30-36 months; n=12 per experiment). The children were asked to indicate which of two bouncing balls was making a centrally located sound. The balls varied in size and/or surface darkness; the sound varied in pitch. The children reliably matched the higher pitched sound to a smaller and lighter (white) ball (Experiment 1), to a lighter (white) ball (Experiment 2), and in one of two groups, to a smaller ball (Experiment 3). Children's matching of pitch and size cannot be attributed to intensity matching or to learning. These data support the hypothesis that some cross-modal correspondences may be remnants of the neural mechanisms underlying neonatal perception.}
}
@article{Spence2011,
	title        = {Crossmodal correspondences: A tutorial review},
	author       = {Spence, Charles},
	year         = 2011,
	month        = may,
	day          = {01},
	journal      = {Attention, Perception, {\&} Psychophysics},
	number       = 4,
	pages        = {971--995},
	abstract     = {In many everyday situations, our senses are bombarded by many different unisensory signals at any given time. To gain the most veridical, and least variable, estimate of environmental stimuli/properties, we need to combine the individual noisy unisensory perceptual estimates that refer to the same object, while keeping those estimates belonging to different objects or events separate. How, though, does the brain ``know'' which stimuli to combine? Traditionally, researchers interested in the crossmodal binding problem have focused on the roles that spatial and temporal factors play in modulating multisensory integration. However, crossmodal correspondences between various unisensory features (such as between auditory pitch and visual size) may provide yet another important means of constraining the crossmodal binding problem. A large body of research now shows that people exhibit consistent crossmodal correspondences between many stimulus features in different sensory modalities. For example, people consistently match high-pitched sounds with small, bright objects that are located high up in space. The literature reviewed here supports the view that crossmodal correspondences need to be considered alongside semantic and spatiotemporal congruency, among the key constraints that help our brains solve the crossmodal binding problem.}
}
@article{HAUW2023167,
	title        = {Subtitled speech: Phenomenology of tickertape synesthesia},
	author       = {Fabien Hauw and Mohamed {El Soudany} et al.},
	year         = 2023,
	journal      = {Cortex},
	pages        = {167--179},
	keywords     = {Synesthesia, Tickertape synesthesia, Reading},
	abstract     = {With effort, most literate persons can conjure more or less vague visual mental images of the written form of words they are hearing, an ability afforded by the links between sounds, meaning, and letters. However, as first reported by Francis Galton, persons with ticker-tape synesthesia (TTS) automatically perceive in their mind's eye accurate and vivid images of the written form of all utterances which they are hearing. We propose that TTS results from an atypical setup of the brain reading system, with an increased top-down influence of phonology on orthography. As a first descriptive step towards a deeper understanding of TTS, we identified 26 persons with TTS. Participants had to answer to a questionnaire aiming to describe the phenomenology of TTS along multiple dimensions, including visual and temporal features, triggering stimuli, voluntary control, interference with language processing, etc. We also assessed the synesthetic percepts elicited experimentally by auditory stimuli such as non-speech sounds, pseudowords, and words with various types of correspondence between sounds and letters. We discuss the potential cerebral substrates of those features, argue that TTS may provide a unique window in the mechanisms of written language processing and acquisition, and propose an agenda for future research.}
}
@article{9018269,
	title        = {Toward Image-to-Tactile Cross-Modal Perception for Visually Impaired People},
	author       = {Liu, Huaping and Guo, Di et al.},
	year         = 2021,
	journal      = {IEEE Transactions on Automation Science and Engineering},
	number       = 2,
	pages        = {521--529},
	keywords     = {Generative adversarial networks;Visual impairment;Cross-modal perception;generative adversarial networks (GANs);visually impaired}
}
@inproceedings{7780459,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {He, Kaiming and Zhang, Xiangyu et al.},
	year         = 2016,
	booktitle    = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {770--778},
	keywords     = {Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation}
}
@article{van2008visualizing,
	title        = {Visualizing data using t-SNE.},
	author       = {Van der Maaten, Laurens and Hinton, Geoffrey},
	year         = 2008,
	journal      = {Journal of machine learning research},
	number       = 11
}
@article{wei2022perception,
	title        = {Perception-aware cross-modal signal reconstruction: From audio-haptic to visual},
	author       = {Wei, Xin and Yao, Yuyuan and Wang, Haoyu and Zhou, Liang},
	year         = 2022,
	journal      = {IEEE Transactions on Multimedia},
	publisher    = {Ieee},
	pages        = {5527--5538}
}
@inproceedings{10.1145/3581783.3611696,
	title        = {Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration},
	author       = {Cheng, Harry and Guo, Yangyang and Nie, Liqiang and Cheng, Zhiyong and Kankanhalli, Mohan},
	year         = 2023,
	booktitle    = {Proceedings of the 31st ACM International Conference on Multimedia},
	location     = {Ottawa ON, Canada},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	series       = {MM '23},
	pages        = {7101–7110},
	doi          = {10.1145/3581783.3611696},
	isbn         = 9798400701085,
	abstract     = {Training an effective video action recognition model poses significant computational challenges, particularly under limited resource budgets. Current methods primarily aim to either reduce model size or utilize pre-trained models, limiting their adaptability to various backbone architectures. This paper investigates the issue of over-sampled frames, a prevalent problem in many approaches yet it has received relatively little attention. Despite the use of fewer frames being a potential solution, this approach often results in a substantial decline in performance. To address this issue, we propose a novel method to restore the intermediate features for two sparsely sampled and adjacent video frames. This feature restoration technique brings a negligible increase in computational requirements compared to resource-intensive image encoders, such as ViT. To evaluate the effectiveness of our method, we conduct extensive experiments on four public datasets, including Kinetics-400, ActivityNet, UCF-101, and HMDB-51. With the integration of our method, the efficiency of three commonly used baselines has been improved by over 50\%, with a mere 0.5\% reduction in recognition accuracy. In addition, our method also surprisingly helps improve the generalization ability of the models under zero-shot settings.},
	keywords     = {frame restoration, cross-modal learning, contrastive learning, action recognition}
}
@article{redcore,
	title        = {RedCore: Relative Advantage Aware Cross-Modal Representation Learning for Missing Modalities with Imbalanced Missing Rates},
	author       = {Sun, Jun and Zhang, Xinxin and Han, Shoukang and Ruan, Yu-Ping and Li, Taihao},
	year         = 2024,
	month        = {Mar.},
	journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
	number       = 13,
	pages        = {15173--15182},
	doi          = {10.1609/aaai.v38i13.29440},
	abstractnote = {Multimodal learning is susceptible to modality missing, which poses a major obstacle for its practical applications and, thus, invigorates increasing research interest. In this paper, we investigate two challenging problems: 1) when modality missing exists in the training data, how to exploit the incomplete samples while guaranteeing that they are properly supervised? 2) when the missing rates of different modalities vary, causing or exacerbating the imbalance among modalities, how to address the imbalance and ensure all modalities are well-trained. To tackle these two challenges, we first introduce the variational information bottleneck (VIB) method for the cross-modal representation learning of missing modalities, which capitalizes on the available modalities and the labels as supervision. Then, accounting for the imbalanced missing rates, we define relative advantage to quantify the advantage of each modality over others. Accordingly, a bi-level optimization problem is formulated to adaptively regulate the supervision of all modalities during training. As a whole, the proposed approach features Relative advantage aware Cross-modal representation learning (abbreviated as RedCore) for missing modalities with imbalanced missing rates. Extensive empirical results demonstrate that RedCore outperforms competing models in that it exhibits superior robustness against either large or imbalanced missing rates. The code is available at: https://github.com/sunjunaimer/RedCore.}
}
@article{He2024Multimodal,
	title        = {Multimodal Dialogue Systems via Capturing Context-aware Dependencies and Ordinal Information of Semantic Elements},
	author       = {He, Weidong and Li, Zhi and Wang, Hao and Xu, Tong and Wang, Zhefeng and Huai, Baoxing and Yuan, Nicholas Jing and Chen, Enhong},
	year         = 2024,
	month        = apr,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 3,
	doi          = {10.1145/3645099},
	issue_date   = {June 2024},
	abstract     = {The topic of multimodal conversation systems has recently garnered significant attention across various industries, including travel and retail, among others. While pioneering works in this field have shown promising performance, they often focus solely on context information at the utterance level, overlooking the context-aware dependencies of multimodal semantic elements like words and images. Furthermore, the ordinal information of images, which indicates the relevance between visual context and users’ demands, remains underutilized during the integration of visual content. Additionally, the exploration of how to effectively utilize corresponding attributes provided by users when searching for desired products is still largely unexplored. To address these challenges, we propose PMATE, a Position-aware Multimodal diAlogue system with semanTic Elements. Specifically, to obtain semantic representations at the element level, we first unfold the multimodal historical utterances and devise a position-aware multimodal element-level encoder. This component considers all images that may be relevant to the current turn and introduces a novel position-aware image selector to choose related images before fusing the information from the two modalities. Finally, we present a knowledge-aware two-stage decoder and an attribute-enhanced image searcher for the tasks of generating textual responses and selecting image responses, respectively. We extensively evaluate our model on two large-scale multimodal dialogue datasets, and the results of our experiments demonstrate that our approach outperforms several baseline methods.},
	articleno    = 45,
	keywords     = {Multimodal dialogue system, natural language generation, conversational image search}
}
@article{Ang2024Temporal,
	title        = {Temporal Implicit Multimodal Networks for Investment and Risk Management},
	author       = {Ang, Gary and Lim, Ee-Peng},
	year         = 2024,
	month        = mar,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 2,
	doi          = {10.1145/3643855},
	issue_date   = {April 2024},
	abstract     = {Many deep learning works on financial time-series forecasting focus on predicting future prices/returns of individual assets with numerical price-related information for trading, and hence propose models designed for univariate, single-task, and/or unimodal settings. Forecasting for investment and risk management involves multiple tasks in multivariate settings: forecasts of expected returns and risks of assets in portfolios, and correlations between these assets. As different sources/types of time-series influence future returns, risks, and correlations of assets in different ways, it is also important to capture time-series from different modalities. Hence, this article addresses financial time-series forecasting for investment and risk management in a multivariate, multitask, and multimodal setting. Financial time-series forecasting, however, is challenging due to the low signal-to-noise ratios typical in financial time-series, and as intra-series and inter-series relationships of assets evolve across time. To address these challenges, our proposed Temporal Implicit Multimodal Network (TIME) model learns implicit inter-series relationship networks between assets from multimodal financial time-series at multiple time-steps adaptively. TIME then uses dynamic network and temporal encoding modules to jointly capture such evolving relationships, multimodal financial time-series, and temporal representations. Our experiments show that TIME outperforms other state-of-the-art models on multiple forecasting tasks and investment and risk management applications.},
	articleno    = 38,
	keywords     = {Time-series, forecasting, graphs, graph neural networks, finance, multi-modality}
}
@article{Wang2023Customer,
	title        = {Customer Volume Prediction Using Fusion of Shared-private Dynamic Weighting over Multiple Modalities},
	author       = {Wang, Wenshan and Yang, Su and Zhang, Weishan},
	year         = 2023,
	month        = mar,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 3,
	doi          = {10.1145/3579826},
	issue_date   = {June 2023},
	abstract     = {Customer volume prediction is crucial for a variety of urban applications, such as store location selection. So far, the key challenge lies in how to fuse multiple modalities from different data sources, on account of the massive amount of data accessible, for example, spatio-temporal data and satellite images. In this article, we investigate three dynamic weighting ensemble learning models to fuse spatio-temporal features and visual features for predicting customer volume in the urban commercial district of interest. Specifically, we propose the shared-private dynamic weighting model by incorporating graph neural networks, which is proposed to capture geographic dependencies (i.e., competitiveness or dependencies) between urban commercial districts in an end-to-end manner. To the best of our knowledge, it is the first work to utilize graph neural networks to model such geographic relationships. We conduct a series of experiments to demonstrate the effectiveness of the proposed models based on two real datasets. Furthermore, an elaborated visualization method is performed for knowledge discovery.},
	articleno    = 42,
	keywords     = {Multimodal fusion, dynamic weighting, ensemble learning}
}
@article{Hu2021Identifying,
	title        = {Identifying Illicit Drug Dealers on Instagram with Large-scale Multimodal Data Fusion},
	author       = {Hu, Chuanbo and Yin, Minglei and Liu, Bin and Li, Xin and Ye, Yanfang},
	year         = 2021,
	month        = sep,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 5,
	doi          = {10.1145/3472713},
	issue_date   = {October 2021},
	abstract     = {Illicit drug trafficking via social media sites such as Instagram have become a severe problem, thus drawing a great deal of attention from law enforcement and public health agencies. How to identify illicit drug dealers from social media data has remained a technical challenge for the following reasons. On the one hand, the available data are limited because of privacy concerns with crawling social media sites; on the other hand, the diversity of drug dealing patterns makes it difficult to reliably distinguish drug dealers from common drug users. Unlike existing methods that focus on posting-based detection, we propose to tackle the problem of illicit drug dealer identification by constructing a large-scale multimodal dataset named Identifying Drug Dealers on Instagram (IDDIG). Nearly 4,000 user accounts, of which more than 1,400 are drug dealers, have been collected from Instagram with multiple data sources including post comments, post images, homepage bio, and homepage images. We then design a quadruple-based multimodal fusion method to combine the multiple data sources associated with each user account for drug dealer identification. Experimental results on the constructed IDDIG dataset demonstrate the effectiveness of the proposed method in identifying drug dealers (almost 95\% accuracy). Moreover, we have developed a hashtag-based community detection technique for discovering evolving patterns, especially those related to geography and drug types.},
	articleno    = 59,
	keywords     = {Drug trafficking, drug dealer, Instagram, multimodal data fusion}
}
@article{Bano2024FedCMD,
	title        = {FedCMD: A Federated Cross-modal Knowledge Distillation for Drivers’ Emotion Recognition},
	author       = {Bano, Saira and Tonellotto, Nicola and Cassar\`{a}, Pietro and Gotta, Alberto},
	year         = 2024,
	month        = may,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 3,
	doi          = {10.1145/3650040},
	issue_date   = {June 2024},
	abstract     = {Emotion recognition has attracted a lot of interest in recent years in various application areas such as healthcare and autonomous driving. Existing approaches to emotion recognition are based on visual, speech, or psychophysiological signals. However, recent studies are looking at multimodal techniques that combine different modalities for emotion recognition. In this work, we address the problem of recognizing the user’s emotion as a driver from unlabeled videos using multimodal techniques. We propose a collaborative training method based on cross-modal distillation, i.e., “FedCMD” (Federated Cross-Modal Distillation). Federated Learning (FL) is an emerging collaborative decentralised learning technique that allows each participant to train their model locally to build a better generalized global model without sharing their data. The main advantage of FL is that only local data is used for training, thus maintaining privacy and providing a secure and efficient emotion recognition system. The local model in FL is trained for each vehicle device with unlabeled video data by using sensor data as a proxy. Specifically, for each local model, we show how driver emotional annotations can be transferred from the sensor domain to the visual domain by using cross-modal distillation. The key idea is based on the observation that a driver’s emotional state indicated by a sensor correlates with facial expressions shown in videos. The proposed “FedCMD” approach is tested on the multimodal dataset “BioVid Emo DB” and achieves state-of-the-art performance. Experimental results show that our approach is robust to non-identically distributed data, achieving 96.67\% and 90.83\% accuracy in classifying five different emotions with IID (independently and identically distributed) and non-IID data, respectively. Moreover, our model is much more robust to overfitting, resulting in better generalization than the other existing methods.},
	articleno    = 57,
	keywords     = {Emotion recognition, cross-modal distillation, federated learning, transfer learning}
}
@article{Yin2023Hybrid,
	title        = {Hybrid Representation and Decision Fusion towards Visual-textual Sentiment},
	author       = {Yin, Chunyong and Zhang, Sun and Zeng, Qingkui},
	year         = 2023,
	month        = apr,
	journal      = {ACM Trans. Intell. Syst. Technol.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 3,
	doi          = {10.1145/3583076},
	issue_date   = {June 2023},
	abstract     = {The rising use of online media has changed social customs of the public. Users have become gradually accustomed to sharing daily experiences and publishing personal opinions on social networks. Social data carrying with emotions and attitudes have provided significant decision support for numerous tasks in sentiment analysis. Conventional sentiment analysis methods only concern about textual modality and are vulnerable to the multimodal scenario, while common multimodal approaches only focus on the interactive relationship between modalities without considering unique intra-modal information. A hybrid fusion network is proposed in this work to capture both the inter-modal and intra-modal features. First, in the intermediate fusion stage, a multi-head visual attention is proposed to extract accurate semantic and sentimental information from textual embedding representations with the assistance of visual features. Then, multiple base classifiers are trained to learn independent and diverse discriminative information from different modal representations in the late fusion stage. The final decision is determined based on fusing the decision supports from base classifiers via a decision fusion method. To improve the generalization of our hybrid fusion network, a similarity loss is employed to inject decision diversity into the whole model. Empirical results on multimodal datasets have demonstrated the proposed model achieves a higher accuracy and better generalization compared with baselines for multimodal sentiment analysis.},
	articleno    = 48,
	keywords     = {social network, representation fusion, multimodal, Decision fusion}
}
@article{Chen2023Dialog,
	title        = {Multimodal Dialog Systems with Dual Knowledge-enhanced Generative Pretrained Language Model},
	author       = {Chen, Xiaolin and Song, Xuemeng and Jing, Liqiang and Li, Shuo and Hu, Linmei and Nie, Liqiang},
	year         = 2023,
	month        = nov,
	journal      = {ACM Trans. Inf. Syst.},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	number       = 2,
	doi          = {10.1145/3606368},
	issue_date   = {March 2024},
	abstract     = {Text response generation for multimodal task-oriented dialog systems, which aims to generate the proper text response given the multimodal context, is an essential yet challenging task. Although existing efforts have achieved compelling success, they still suffer from two pivotal limitations: (1)&nbsp;overlook the benefit of generative pretraining and (2) ignore the textual context-related knowledge. To address these limitations, we propose a novel dual knowledge-enhanced generative pretrained language mode for multimodal task-oriented dialog systems&nbsp;(DKMD), consisting of three key components: dual knowledge selection, dual knowledge-enhanced context learning, and knowledge-enhanced response generation. To be specific, the dual knowledge selection component aims to select the related knowledge according to both textual and visual modalities of the given context. Thereafter, the dual knowledge-enhanced context learning component targets seamlessly, integrating the selected knowledge into the multimodal context learning from both global and local perspectives, where the cross-modal semantic relation is also explored. Moreover, the knowledge-enhanced response generation component comprises a revised BART decoder, where an additional dot-product knowledge-decoder attention sub-layer is introduced for explicitly utilizing the knowledge to advance the text response generation. Extensive experiments on a public dataset verify the superiority of the proposed DKMD over state-of-the-art competitors.},
	articleno    = 53,
	keywords     = {Multimodal task-oriented dialog systems; text response generation; generative pretrained language model; dual knowledge selection}
}
@inproceedings{NEURIPS2023_9156b0f6,
 author = {Chen, Minshuo and Bai, Yu and Poor, H. Vincent and Wang, Mengdi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {46390--46418},
 publisher = {Curran Associates, Inc.},
 title = {Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/9156b0f6dfa9bbd18c79cc459ef5d61c-Paper-Conference.pdf},
 volume = {36},
 year = {2023},
    abstract={In real-world reinforcement learning (RL) systems, various forms of impaired
observability can complicate matters. These situations arise when an agent is
unable to observe the most recent state of the system due to latency or lossy
channels, yet the agent must still make real-time decisions. This paper introduces
a theoretical investigation into efficient RL in control systems where agents must
act with delayed and missing state observations. We establish near-optimal regret
bounds, of the form eO(ppoly(H)SAK), for RL in both the delayed and missing
observation settings. Despite impaired observability posing significant challenges
to the policy class and planning, our results demonstrate that learning remains
efficient, with the regret bound optimally depending on the state-action size of the
original system. Additionally, we provide a characterization of the performance of
the optimal policy under impaired observability, comparing it to the optimal value
obtained with full observability}
}
@ARTICLE{9043893,
  author={Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
  journal={IEEE Transactions on Cybernetics}, 
  title={Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications}, 
  year={2020},
  volume={50},
  number={9},
  pages={3826-3839},
  abstract={Reinforcement learning (RL) algorithms have been around for decades and employed to solve various sequential decision-making problems. These algorithms, however, have faced great challenges when dealing with high-dimensional environments. The recent development of deep learning has enabled RL methods to drive optimal policies for sophisticated and capable agents, which can perform efficiently in these challenging environments. This article addresses an important aspect of deep RL related to situations that require multiple agents to communicate and cooperate to solve complex tasks. A survey of different approaches to problems related to multiagent deep RL (MADRL) is presented, including nonstationarity, partial observability, continuous state and action spaces, multiagent training schemes, and multiagent transfer learning. The merits and demerits of the reviewed methods will be analyzed and discussed with their corresponding applications explored. It is envisaged that this review provides insights about various MADRL methods and can lead to the future development of more robust and highly useful multiagent learning methods for solving real-world problems.},
  keywords={Mathematical model;Robots;Dynamic programming;Games;Reinforcement learning;Deep learning;Observability;Continuous action space;deep learning;deep reinforcement learning (RL);multiagent;nonstationary;partial observability;review;robotics;survey},
  doi={10.1109/TCYB.2020.2977374},
  ISSN={2168-2275},
  month={Sep.},}
@misc{li2020federatedoptimizationheterogeneousnetworks,
      title={Federated Optimization in Heterogeneous Networks}, 
      author={Tian Li and Anit Kumar Sahu and Manzil Zaheer and Maziar Sanjabi and Ameet Talwalkar and Virginia Smith},
      year={2020},
      eprint={1812.06127},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.06127}, 
}
@ARTICLE{adnan_cfl_2022,
  author={Qayyum, Adnan and Ahmad, Kashif and Ahsan, Muhammad Ahtazaz and Al-Fuqaha, Ala and Qadir, Junaid},
  journal={IEEE Open Journal of the Computer Society}, 
  title={Collaborative Federated Learning for Healthcare: Multi-Modal COVID-19 Diagnosis at the Edge}, 
  year={2022},
  volume={3},
  number={},
  pages={172-184},
  abstract={Despite significant improvements over the last few years, cloud-based healthcare applications continue to suffer from poor adoption due to their limitations in meeting stringent security, privacy, and quality of service requirements (such as low latency). The edge computing trend, along with techniques for distributed machine learning such as federated learning, has gained popularity as a viable solution in such settings. In this paper, we leverage the capabilities of edge computing in medicine by evaluating the potential of intelligent processing of clinical data at the edge. We utilized the emerging concept of clustered federated learning (CFL) for an automatic COVID-19 diagnosis. We evaluate the performance of the proposed framework under different experimental setups on two benchmark datasets. Promising results are obtained on both datasets resulting in comparable results against the central baseline where the specialized models (i.e., each on a specific image modality) are trained with central data, and improvements of 16% and 11% in overall F1-Scores have been achieved over the trained model trained (using multi-modal COVID-19 data) in the CFL setup on X-ray and Ultrasound datasets, respectively. We also discussed the associated challenges, technologies, and techniques available for deploying ML at the edge in such privacy and delay-sensitive applications.},
  keywords={COVID-19;Computed tomography;Medical services;X-ray imaging;Feature extraction;Collaborative work;Data models;Distributed computing;machine learning;smart healthcare},
  doi={10.1109/OJCS.2022.3206407},
  ISSN={2644-1268},
  month={},}
@inproceedings{pretrained_multimodal_decision_making_yunhao_24,
	title        = {Multimodal Pretrained Models for Verifiable Sequential Decision-Making: Planning, Grounding, and Perception},
	author       = {Yang, Yunhao and Neary, Cyrus and Topcu, Ufuk},
	year         = 2024,
	booktitle    = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
	location     = {Auckland, New Zealand},
	publisher    = {International Foundation for Autonomous Agents and Multiagent Systems},
	address      = {Richland, SC},
	series       = {AAMAS '24},
	pages        = {2011–2019},
	isbn         = 9798400704864,
	abstract     = {Recently developed pretrained models can encode rich world knowledge expressed in multiple modalities, such as text and images. However, the outputs of these models cannot be integrated into algorithms to solve sequential decision-making tasks. We develop an algorithm that utilizes the knowledge from pretrained models to construct and verify controllers for sequential decision-making tasks, and to ground these controllers to task environments through visual observations with formal guarantees. In particular, the algorithm queries a pretrained model with a user-provided, text-based task description and uses the model's output to construct an automaton-based controller that encodes the model's task-relevant knowledge. It allows formal verification of whether the knowledge encoded in the controller is consistent with other independently available knowledge, which may include abstract information on the environment or user-provided specifications. Next, the algorithm leverages the vision and language capabilities of pretrained models to link the observations from the task environment to the text-based control logic from the controller (e.g., actions and conditions that trigger the actions). We propose a mechanism to provide probabilistic guarantees on whether the controller satisfies the user-provided specifications under perceptual uncertainties. We demonstrate the algorithm's ability to construct, verify, and ground automaton-based controllers through a suite of real-world tasks, including daily life and robot manipulation tasks.},
	numpages     = 9,
	keywords     = {automaton-based representation, formal methods, multimodal pretrained model, perception, sequential decision-making, verification}
}


